<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh_Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css">


  <meta name="keywords" content="python,problem,machinelearning,">








  <link rel="shortcut icon" type="image/x-icon" href="/images/avatar.jpg?v=5.1.2">






<meta name="description" content="本来想着放在笔记后面，后来发现好像问题有点多…决定新开一篇文章来写 hw1https://www.kaggle.com/c/ml2019spring-hw1/overview作业说明 我一开始做了一个非常naive的model，反正分开处理，Python用的也不是很熟练，就当练代码了一开始没用AdaGrad，发现Learning Rate真的很难设置如果数据量小，那么Learning Rate应该">
<meta name="keywords" content="python,problem,machinelearning">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习练习">
<meta property="og:url" content="beiyuouo.github.io/2019/07/17/ml-practice/index.html">
<meta property="og:site_name" content="北屿">
<meta property="og:description" content="本来想着放在笔记后面，后来发现好像问题有点多…决定新开一篇文章来写 hw1https://www.kaggle.com/c/ml2019spring-hw1/overview作业说明 我一开始做了一个非常naive的model，反正分开处理，Python用的也不是很熟练，就当练代码了一开始没用AdaGrad，发现Learning Rate真的很难设置如果数据量小，那么Learning Rate应该">
<meta property="og:locale" content="zh_Hans">
<meta property="og:image" content="/img/bqb/naotou.gif">
<meta property="og:image" content="/img/ml-practice/hw3/pic0.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/pic1.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/pic2.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/pic3.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/pic4.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/pic5.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/pic6.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/pic7.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/pic8.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/pic9.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/test_pic0.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/test_pic1.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/test_pic2.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/test_pic3.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/test_pic4.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/test_pic5.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/test_pic6.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/test_pic7.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/test_pic8.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/test_pic9.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/showpic0.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/showpic1.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/showpic2.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/showpic3.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/showpic4.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/showpic5.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/showpic6.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/showpic7.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/showpic8.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/showpic9.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/showpic10.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/showpic11.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/showpic12.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/showpic13.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/showpic14.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/showpic15.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/showpic16.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/showpic17.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/showpic18.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/showpic19.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/showpic20.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/showpic21.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/showpic22.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/showpic23.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/showpic24.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/showpic25.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/showpic26.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/showpic27.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/showpic28.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/showpic29.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/showpic30.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/showpic31.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/pic0.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/showpic32.jpg">
<meta property="og:image" content="/img/ml-practice/hw3/showpic32.jpg">
<meta property="og:updated_time" content="2019-08-02T14:03:19.365Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习练习">
<meta name="twitter:description" content="本来想着放在笔记后面，后来发现好像问题有点多…决定新开一篇文章来写 hw1https://www.kaggle.com/c/ml2019spring-hw1/overview作业说明 我一开始做了一个非常naive的model，反正分开处理，Python用的也不是很熟练，就当练代码了一开始没用AdaGrad，发现Learning Rate真的很难设置如果数据量小，那么Learning Rate应该">
<meta name="twitter:image" content="/img/bqb/naotou.gif">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="beiyuouo.github.io/2019/07/17/ml-practice/">







  <title>机器学习练习 | 北屿</title>
  




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-145083103-1', 'auto');
  ga('send', 'pageview');
</script>


  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d396c00ca4fe6547a19249d34cb91254";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->










</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh_Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">北屿</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">北屿小智障</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-game">
          <a href="/game/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-gamepad"></i> <br>
            
            game
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="Searching..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="beiyuouo.github.io/2019/07/17/ml-practice/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="北屿">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="北屿">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">机器学习练习</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-07-17T22:20:33+08:00">
                2019-07-17
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2019-08-02T22:03:19+08:00">
                2019-08-02
              </time>
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/problem/" itemprop="url" rel="index">
                    <span itemprop="name">problem</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>
            </span>
          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  10.1k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">
                  58
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>本来想着放在笔记后面，后来发现好像问题有点多…决定新开一篇文章来写</p>
<h2 id="hw1"><a href="#hw1" class="headerlink" title="hw1"></a>hw1</h2><p><a href="https://www.kaggle.com/c/ml2019spring-hw1/overview" target="_blank" rel="noopener">https://www.kaggle.com/c/ml2019spring-hw1/overview</a><br><a href="https://ntumlta2019.github.io/ml-web-hw1/" target="_blank" rel="noopener">作业说明</a></p>
<p>我一开始做了一个非常naive的model，反正分开处理，Python用的也不是很熟练，就当练代码了<br>一开始没用AdaGrad，发现Learning Rate真的很难设置<br>如果数据量小，那么Learning Rate应该大一些，如果数据量大，那就得小一些了，这个都是相对的<br>如果数据本身小，那么初始值就没那么重要，如果本身大的话，那初始值就需要自己手动设置一下了<br>举个第一个的栗子，如果一开始初始值都设成0,12960条数据，一开始得到的Loss就有51361098.87099994，Learning Rate我设成了1e-8，得到的结果还越来越大…<br>当我把数据量调小一点，比如20条这个样子，还是有点用的…Learning Rate = 1e-6，迭代50次大概能得到一个不错的结果…<br>实测<code>1e-9,1e-12</code>时有点用了…<br>我忽然有个大胆的想法…如果前面变化大点后面变化小点是不是很科学，可以飞快接近结果<br>p.s. Python的输出调试真的不好用…不如c++的<code>#define debug(x) cout&lt;&lt;#x&lt;&lt;&quot;=&quot;&lt;&lt;x&lt;&lt;endl</code>，可能我还没get到该技能<br>原来只需要预测PM2.5就行了，那我直接写全得了<br>一开始Loss=<code>4231600.00</code>，最后<code>313187.27500353276</code></p>
<p>现在我不是很清楚要什么时候结束…所以先直接计次循环了<br>先在这保存一下代码…之后改成AdaGrad<br>果然这个naive的程序得到了<code>private:9.66107 public:8.18926</code>的高分，我觉得以后应该把训练的模块和输出模块分开来写，不然必须训练一次才能输出233<br>第一次训练出来的w和b</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">w = [<span class="number">0.02560433358938389</span>, <span class="number">0.012801930896077247</span>, <span class="number">0.004710981594648449</span>, <span class="number">0.005885507375027003</span>, <span class="number">0.009942499746330985</span>, <span class="number">0.026670961533262413</span>, <span class="number">-0.0010603121145215564</span>, <span class="number">0.25976948883940576</span>, <span class="number">0.6202885065990642</span>]</span><br><span class="line">b = <span class="number">0.01483973260431497</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># read data</span></span><br><span class="line"></span><br><span class="line">ff = open(<span class="string">'train.csv'</span>, <span class="string">'r'</span>, <span class="keyword">True</span>, <span class="string">'big5hkscs'</span>);</span><br><span class="line">data_init = ff.readlines()</span><br><span class="line">total_line = len(data_init)<span class="number">-1</span></span><br><span class="line"><span class="comment"># total_line = 19</span></span><br><span class="line">total_type = <span class="number">8</span></span><br><span class="line"></span><br><span class="line">data = [[] <span class="keyword">for</span> i <span class="keyword">in</span> range(total_type)]</span><br><span class="line"><span class="comment"># print(data)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, total_line+<span class="number">1</span>): </span><br><span class="line">    <span class="comment"># print(i, data_init[i], end=' ')</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> data_init[i].split(<span class="string">','</span>)[<span class="number">3</span>:]: </span><br><span class="line">        <span class="comment"># print(j, end=' ')</span></span><br><span class="line">        <span class="keyword">if</span>(j.startswith(<span class="string">"NR"</span>)):</span><br><span class="line">            data[(i<span class="number">-1</span>)%total_type].append(<span class="number">0.0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            data[(i<span class="number">-1</span>)%total_type].append(float(j))</span><br><span class="line">    <span class="comment"># print()</span></span><br><span class="line"><span class="comment"># print(data)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># for i in data:</span></span><br><span class="line"><span class="comment">#     print(len(i))</span></span><br><span class="line"></span><br><span class="line">ff.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># init model without adagrad</span></span><br><span class="line"><span class="comment"># just consider one type</span></span><br><span class="line">learningrate = <span class="number">1e-11</span></span><br><span class="line">w = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">9</span>)]</span><br><span class="line">b = <span class="number">0</span></span><br><span class="line">par = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">9</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># data[0]=data[0][:20]</span></span><br><span class="line"><span class="comment"># print(data[0])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(a)</span>:</span> <span class="comment"># calc y</span></span><br><span class="line">    y = b</span><br><span class="line">    <span class="comment"># print(len(a))</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(a)):</span><br><span class="line">        y += w[i]*a[i]</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">LF</span><span class="params">()</span>:</span> <span class="comment"># loss function</span></span><br><span class="line">    tot1, tot2 = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">9</span>, len(data[<span class="number">0</span>])<span class="number">-1</span>):</span><br><span class="line">        tot1 += data[<span class="number">0</span>][i]-f(data[<span class="number">0</span>][i<span class="number">-9</span>: i])</span><br><span class="line">        tot2 += (data[<span class="number">0</span>][i]-f(data[<span class="number">0</span>][i<span class="number">-9</span>: i]))**<span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> tot1, tot2</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getpart</span><span class="params">()</span>:</span> <span class="comment"># get partial w_i,b</span></span><br><span class="line">    <span class="keyword">global</span> w,b,learningrate</span><br><span class="line">    parw = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">9</span>)]</span><br><span class="line">    parb = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">9</span>, len(data[<span class="number">0</span>])<span class="number">-1</span>):</span><br><span class="line">        a = data[<span class="number">0</span>][i<span class="number">-9</span>: i]</span><br><span class="line">        tmp = data[<span class="number">0</span>][i]-f(a)</span><br><span class="line">        <span class="comment"># print('a = ', a, ' tmp = ', tmp, 'f(a) = ', f(a))</span></span><br><span class="line">        parb += <span class="number">-2</span>*(tmp)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(parw)):</span><br><span class="line">            parw[j] += <span class="number">-2</span>*tmp*a[j]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># print(parw, parb)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">9</span>):</span><br><span class="line">        w[i]=w[i]-learningrate*parw[i]</span><br><span class="line">    b = b-learningrate*parb</span><br><span class="line"></span><br><span class="line"><span class="comment"># main loop</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    loss1, loss2 = LF()</span><br><span class="line">    print(loss2)</span><br><span class="line">    getpart()</span><br><span class="line">    <span class="comment"># print(w, b, learningrate, end=' -&gt; ')</span></span><br></pre></td></tr></table></figure>
<p>其实我觉得浮点数精度带来的误差还是很大的…<br>今天跑不完了，明早起来再跑吧233</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># read data</span></span><br><span class="line"></span><br><span class="line">ff = open(<span class="string">'train.csv'</span>, <span class="string">'r'</span>, <span class="keyword">True</span>, <span class="string">'big5hkscs'</span>);</span><br><span class="line">data_init = ff.readlines()</span><br><span class="line">total_line = len(data_init)<span class="number">-1</span></span><br><span class="line"><span class="comment"># total_line = 19</span></span><br><span class="line">total_type = <span class="number">18</span></span><br><span class="line"></span><br><span class="line">data = [[] <span class="keyword">for</span> i <span class="keyword">in</span> range(total_type)]</span><br><span class="line"><span class="comment"># print(data)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, total_line+<span class="number">1</span>): </span><br><span class="line">    <span class="comment"># print(i, data_init[i], end=' ')</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> data_init[i].split(<span class="string">','</span>)[<span class="number">3</span>:]: </span><br><span class="line">        <span class="comment"># print(j, end=' ')</span></span><br><span class="line">        <span class="keyword">if</span>(j.startswith(<span class="string">"NR"</span>)):</span><br><span class="line">            data[(i<span class="number">-1</span>)%total_type].append(<span class="number">0.0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            data[(i<span class="number">-1</span>)%total_type].append(float(j))</span><br><span class="line">    <span class="comment"># print()</span></span><br><span class="line"><span class="comment"># print(data)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># for i in data:</span></span><br><span class="line"><span class="comment">#     print(len(i))</span></span><br><span class="line"></span><br><span class="line">ff.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># init model without adagrad</span></span><br><span class="line"><span class="comment"># just consider one type</span></span><br><span class="line">learningrate = <span class="number">1e-9</span></span><br><span class="line"><span class="comment"># w = [0 for i in range(9)]</span></span><br><span class="line"><span class="comment"># b = 0</span></span><br><span class="line"></span><br><span class="line">w = [<span class="number">0.025824524413455778</span>, <span class="number">0.013532431831135698</span>, <span class="number">0.006330265404890848</span>, <span class="number">0.007117255928494381</span>, <span class="number">0.01912520613272634</span>, <span class="number">0.031712687701763935</span>, <span class="number">-0.1291674302877186</span>, <span class="number">0.21741455874560975</span>, <span class="number">0.7772617955877607</span>]</span><br><span class="line">b = <span class="number">0.02353711727271333</span></span><br><span class="line"></span><br><span class="line">par = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">9</span>)]</span><br><span class="line"></span><br><span class="line">print(data[<span class="number">9</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(a)</span>:</span> <span class="comment"># calc y</span></span><br><span class="line">    y = b</span><br><span class="line">    <span class="comment"># print(len(a))</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(a)):</span><br><span class="line">        y += w[i]*a[i]</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">LF</span><span class="params">()</span>:</span> <span class="comment"># loss function</span></span><br><span class="line">    tot1, tot2 = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">9</span>, len(data[<span class="number">9</span>])<span class="number">-1</span>):</span><br><span class="line">        tot1 += data[<span class="number">9</span>][i]-f(data[<span class="number">9</span>][i<span class="number">-9</span>: i])</span><br><span class="line">        tot2 += (data[<span class="number">9</span>][i]-f(data[<span class="number">9</span>][i<span class="number">-9</span>: i]))**<span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> tot1, tot2</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getpart</span><span class="params">()</span>:</span> <span class="comment"># get partial w_i,b</span></span><br><span class="line">    <span class="keyword">global</span> w,b,learningrate</span><br><span class="line">    parw = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">9</span>)]</span><br><span class="line">    parb = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">9</span>, len(data[<span class="number">9</span>])<span class="number">-1</span>):</span><br><span class="line">        a = data[<span class="number">9</span>][i<span class="number">-9</span>: i]</span><br><span class="line">        tmp = data[<span class="number">9</span>][i]-f(a)</span><br><span class="line">        <span class="comment"># print('a = ', a, ' tmp = ', tmp, 'f(a) = ', f(a))</span></span><br><span class="line">        parb += <span class="number">-2</span>*(tmp)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(parw)):</span><br><span class="line">            parw[j] += <span class="number">-2</span>*tmp*a[j]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># print(parw, parb)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">9</span>):</span><br><span class="line">        w[i]=w[i]-learningrate*parw[i]</span><br><span class="line">    b = b-learningrate*parb</span><br><span class="line"></span><br><span class="line"><span class="comment"># main loop</span></span><br><span class="line">times = <span class="number">1000</span></span><br><span class="line">learningrate = <span class="number">1e-9</span></span><br><span class="line">loss2 = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">    lst = loss2</span><br><span class="line">    loss1, loss2 = LF()</span><br><span class="line">    print(loss2)</span><br><span class="line">    getpart()</span><br><span class="line">    <span class="keyword">if</span> abs(lst-loss2) &lt; <span class="number">0.1</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="comment"># print(w, b, learningrate, end=' -&gt; ')</span></span><br><span class="line"></span><br><span class="line">learningrate = <span class="number">1e-10</span></span><br><span class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">    lst = loss2</span><br><span class="line">    loss1, loss2 = LF()</span><br><span class="line">    print(loss2)</span><br><span class="line">    getpart()</span><br><span class="line">    <span class="keyword">if</span> abs(lst-loss2) &lt; <span class="number">0.1</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">learningrate = <span class="number">1e-11</span></span><br><span class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">    lst = loss2</span><br><span class="line">    loss1, loss2 = LF()</span><br><span class="line">    print(loss2)</span><br><span class="line">    getpart()</span><br><span class="line">    <span class="keyword">if</span> abs(lst-loss2) &lt; <span class="number">0.1</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(w, b)</span><br><span class="line">print(<span class="string">'----------------------'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># read test data &amp; output result</span></span><br><span class="line">ff = open(<span class="string">'test.csv'</span>, <span class="string">'r'</span>, <span class="keyword">True</span>, <span class="string">'big5hkscs'</span>)</span><br><span class="line">fo = open(<span class="string">'ans.csv'</span>, <span class="string">'w'</span>, <span class="keyword">True</span>, <span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line">dd = ff.readlines()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'id,value'</span>, file=fo)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> dd:</span><br><span class="line">    line = i.split(<span class="string">','</span>)</span><br><span class="line">    <span class="comment"># print(line)</span></span><br><span class="line">    <span class="keyword">if</span>(line[<span class="number">1</span>].startswith(<span class="string">"PM2.5"</span>)):</span><br><span class="line">        a = []</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> line[<span class="number">2</span>:]:</span><br><span class="line">            a.append(float(j))</span><br><span class="line">        print(line[<span class="number">0</span>], <span class="string">','</span>, f(a), file=fo, sep=<span class="string">''</span>)</span><br></pre></td></tr></table></figure>
<p>总共跑了大概30min，然后得到了一个结果<code>226303.2859914291</code>，得到的w和b在下面，得到的分数是<code>private:7.29680 public:6.02679</code>，也差不多是我这种乱搞做法的比较好的结果了吧<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">w = [<span class="number">-0.01196089473848148</span>, <span class="number">-0.0002154942523566853</span>, <span class="number">0.15614885717614674</span>, <span class="number">-0.18703390611436793</span>, <span class="number">-0.028484991059330843</span>, <span class="number">0.46031602604586386</span>, <span class="number">-0.5230520726349329</span>, <span class="number">0.013331313987351568</span>, <span class="number">1.091108574289711</span>]</span><br><span class="line">b = <span class="number">0.15907341031251657</span></span><br></pre></td></tr></table></figure></p>
<p>我发现那个助教做Demo的时候也是计次…<br>AdaGrad 有进步但是不太大<code>private:7.24162 public:5.93032</code><br>我发现Learning Rate的初始值还是很重要的，不然第一次偏差太大，后面跑回来就比较慢了</p>
<p>我又在这基础上Train了一下，<code>private:7.24797 public:5.92997</code>，分数反而下降了…<br>不解挠头，我好像不知道怎么变得更好了…<br><img src="/img/bqb/naotou.gif" alt=""><br>我觉得吧…可能是多加些参数了，把其他的条件考虑进去再加上二次项</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># read data</span></span><br><span class="line"></span><br><span class="line">ff = open(<span class="string">'train.csv'</span>, <span class="string">'r'</span>, <span class="keyword">True</span>, <span class="string">'big5hkscs'</span>);</span><br><span class="line">data_init = ff.readlines()</span><br><span class="line">total_line = len(data_init)<span class="number">-1</span></span><br><span class="line"><span class="comment"># total_line = 19</span></span><br><span class="line">total_type = <span class="number">18</span></span><br><span class="line"></span><br><span class="line">data = [[] <span class="keyword">for</span> i <span class="keyword">in</span> range(total_type)]</span><br><span class="line"><span class="comment"># print(data)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, total_line+<span class="number">1</span>): </span><br><span class="line">    <span class="comment"># print(i, data_init[i], end=' ')</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> data_init[i].split(<span class="string">','</span>)[<span class="number">3</span>:]: </span><br><span class="line">        <span class="comment"># print(j, end=' ')</span></span><br><span class="line">        <span class="keyword">if</span>(j.startswith(<span class="string">"NR"</span>)):</span><br><span class="line">            data[(i<span class="number">-1</span>)%total_type].append(<span class="number">0.0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            data[(i<span class="number">-1</span>)%total_type].append(float(j))</span><br><span class="line">    <span class="comment"># print()</span></span><br><span class="line"><span class="comment"># print(data)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># for i in data:</span></span><br><span class="line"><span class="comment">#     print(len(i))</span></span><br><span class="line"></span><br><span class="line">ff.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># init model without adagrad</span></span><br><span class="line"><span class="comment"># just consider one type</span></span><br><span class="line"></span><br><span class="line">learningrate = <span class="number">0.02</span></span><br><span class="line"><span class="comment"># w = [0 for i in range(9)]</span></span><br><span class="line"><span class="comment"># b = 0</span></span><br><span class="line"></span><br><span class="line">w = [<span class="number">-0.027453064157575136</span>, <span class="number">-0.023350934972838352</span>, <span class="number">0.20364257875107225</span>, <span class="number">-0.22111429241618002</span>, <span class="number">-0.054197655604759414</span>, <span class="number">0.511059787427728</span>, <span class="number">-0.5551156971312344</span>, <span class="number">0.0029147111942378363</span>, <span class="number">1.0896194827444017</span>]</span><br><span class="line">b = <span class="number">1.516876275197267</span></span><br><span class="line"></span><br><span class="line">par = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">9</span>)]</span><br><span class="line"></span><br><span class="line">g = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">9</span>)]</span><br><span class="line">gb = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">print(data[<span class="number">9</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(a)</span>:</span> <span class="comment"># calc y</span></span><br><span class="line">    y = b</span><br><span class="line">    <span class="comment"># print(len(a))</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(a)):</span><br><span class="line">        y += w[i]*a[i]</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">LF</span><span class="params">()</span>:</span> <span class="comment"># loss function</span></span><br><span class="line">    tot1, tot2 = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">9</span>, len(data[<span class="number">9</span>])<span class="number">-1</span>):</span><br><span class="line">        tot1 += data[<span class="number">9</span>][i]-f(data[<span class="number">9</span>][i<span class="number">-9</span>: i])</span><br><span class="line">        tot2 += (data[<span class="number">9</span>][i]-f(data[<span class="number">9</span>][i<span class="number">-9</span>: i]))**<span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> tot1, tot2</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getpart</span><span class="params">()</span>:</span> <span class="comment"># get partial w_i,b</span></span><br><span class="line">    <span class="keyword">global</span> w, b, learningrate, g, gb</span><br><span class="line">    parw = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">9</span>)]</span><br><span class="line">    parb = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">9</span>, len(data[<span class="number">9</span>])<span class="number">-1</span>):</span><br><span class="line">        a = data[<span class="number">9</span>][i<span class="number">-9</span>: i]</span><br><span class="line">        tmp = data[<span class="number">9</span>][i]-f(a)</span><br><span class="line">        <span class="comment"># print('a = ', a, ' tmp = ', tmp, 'f(a) = ', f(a))</span></span><br><span class="line">        parb += <span class="number">-2</span>*(tmp)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(parw)):</span><br><span class="line">            parw[j] += <span class="number">-2</span>*tmp*a[j]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">9</span>):</span><br><span class="line">        g[i] += parw[i]**<span class="number">2</span></span><br><span class="line">    gb += parb**<span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># print(parw, parb)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">9</span>):</span><br><span class="line">        w[i]=w[i]-learningrate/np.sqrt(g[i])*parw[i]</span><br><span class="line">    b = b-learningrate/np.sqrt(gb)*parb</span><br><span class="line"></span><br><span class="line"><span class="comment"># main loop</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">times = <span class="number">10000</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(times): </span><br><span class="line">    loss1, loss2 = LF()</span><br><span class="line">    print(loss2)</span><br><span class="line">    getpart()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(w, b)</span><br><span class="line">print(<span class="string">'----------------------'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># read test data &amp; output result</span></span><br><span class="line">ff = open(<span class="string">'test.csv'</span>, <span class="string">'r'</span>, <span class="keyword">True</span>, <span class="string">'big5hkscs'</span>)</span><br><span class="line">fo = open(<span class="string">'ans.csv'</span>, <span class="string">'w'</span>, <span class="keyword">True</span>, <span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line">dd = ff.readlines()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'id,value'</span>, file=fo)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> dd:</span><br><span class="line">    line = i.split(<span class="string">','</span>)</span><br><span class="line">    <span class="comment"># print(line)</span></span><br><span class="line">    <span class="keyword">if</span>(line[<span class="number">1</span>].startswith(<span class="string">"PM2.5"</span>)):</span><br><span class="line">        a = []</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> line[<span class="number">2</span>:]:</span><br><span class="line">            a.append(float(j))</span><br><span class="line">        print(line[<span class="number">0</span>], <span class="string">','</span>, f(a), file=fo, sep=<span class="string">''</span>)</span><br></pre></td></tr></table></figure>
<p>哇哦 果然，考虑了二次项，就Training了一次，从0开始，虽然得到的loss有<code>249940.5819514999</code>，但是得到的结果就比之前好得多<code>private:7.19917 public:6.21872</code><br>惊了第二次<code>private:6.64656 public:6.05368</code>，直接过了strong baseline，进前30了…<br>第三次<code>private:6.48045 public:5.98424</code>，嗯，这个作业就先这样吧…继续看看然后搞后面的了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train once</span></span><br><span class="line">w = [<span class="number">0.038419991693316446</span>, <span class="number">0.007631560179476181</span>, <span class="number">0.017121148559214105</span>, <span class="number">0.008416625938076962</span>, <span class="number">0.03344269223900898</span>, <span class="number">0.07485213150840617</span>, <span class="number">0.02749672696408752</span>, <span class="number">0.22824357598542835</span>, <span class="number">0.5376247829222051</span>, <span class="number">-0.0009578224442509027</span>, <span class="number">-5.528884121464584e-05</span>, <span class="number">0.0015892367969355542</span>, <span class="number">-0.002221411021284927</span>, <span class="number">-0.0005837884778601767</span>, <span class="number">0.004221860266845362</span>, <span class="number">-0.006797969291691342</span>, <span class="number">-0.00225338003274975</span>, <span class="number">0.006461549785024542</span>] </span><br><span class="line">b = <span class="number">0.7707164178028746</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># train twice</span></span><br><span class="line">w = [<span class="number">0.04550231793751899</span>, <span class="number">-0.002754055810508499</span>, <span class="number">0.01938652727209127</span>, <span class="number">-0.012606727374062035</span>, <span class="number">0.026644811794796486</span>, <span class="number">0.08529566547520964</span>, <span class="number">-0.07289173105545857</span>, <span class="number">0.20945837091642078</span>, <span class="number">0.6670259504885876</span>, <span class="number">-0.0010213018236748075</span>, <span class="number">5.8566384278349695e-05</span>, <span class="number">0.0015977330997915425</span>, <span class="number">-0.0020381249207644136</span>, <span class="number">-0.0005208828351068215</span>, <span class="number">0.00420797586899282</span>, <span class="number">-0.005646668562707524</span>, <span class="number">-0.0020696412509102606</span>, <span class="number">0.004923568539656863</span>]</span><br><span class="line">b = <span class="number">0.9254806340743505</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># train third</span></span><br><span class="line">w = [<span class="number">0.04570395973486438</span>, <span class="number">-0.011294077186582839</span>, <span class="number">0.025386967127299973</span>, <span class="number">-0.029212382678632333</span>, <span class="number">0.028545066193682748</span>, <span class="number">0.11101330569370138</span>, <span class="number">-0.14814515409168333</span>, <span class="number">0.17550209989307616</span>, <span class="number">0.7622350658920563</span>, <span class="number">-0.0010221349938858725</span>, <span class="number">0.00015306217718345381</span>, <span class="number">0.0015709039506438167</span>, <span class="number">-0.0019038640450172112</span>, <span class="number">-0.0005728314281040582</span>, <span class="number">0.00400234194537975</span>, <span class="number">-0.004792016150808898</span>, <span class="number">-0.001711617920489784</span>, <span class="number">0.0038067167936126386</span>]</span><br><span class="line">b = <span class="number">1.0625348284097278</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># read data</span></span><br><span class="line"></span><br><span class="line">ff = open(<span class="string">'train.csv'</span>, <span class="string">'r'</span>, <span class="keyword">True</span>, <span class="string">'big5hkscs'</span>);</span><br><span class="line">data_init = ff.readlines()</span><br><span class="line">total_line = len(data_init)<span class="number">-1</span></span><br><span class="line"><span class="comment"># total_line = 19</span></span><br><span class="line">total_type = <span class="number">18</span></span><br><span class="line"></span><br><span class="line">data = [[] <span class="keyword">for</span> i <span class="keyword">in</span> range(total_type)]</span><br><span class="line"><span class="comment"># print(data)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, total_line+<span class="number">1</span>): </span><br><span class="line">    <span class="comment"># print(i, data_init[i], end=' ')</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> data_init[i].split(<span class="string">','</span>)[<span class="number">3</span>:]: </span><br><span class="line">        <span class="comment"># print(j, end=' ')</span></span><br><span class="line">        <span class="keyword">if</span>(j.startswith(<span class="string">"NR"</span>)):</span><br><span class="line">            data[(i<span class="number">-1</span>)%total_type].append(<span class="number">0.0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            data[(i<span class="number">-1</span>)%total_type].append(float(j))</span><br><span class="line">    <span class="comment"># print()</span></span><br><span class="line"><span class="comment"># print(data)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># for i in data:</span></span><br><span class="line"><span class="comment">#     print(len(i))</span></span><br><span class="line"></span><br><span class="line">ff.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># init model without adagrad</span></span><br><span class="line"><span class="comment"># just consider one type</span></span><br><span class="line"></span><br><span class="line">learningrate = <span class="number">1</span></span><br><span class="line">w = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">18</span>)]</span><br><span class="line">b = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">g = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">18</span>)]</span><br><span class="line">gb = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># print(data[9])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(a)</span>:</span> <span class="comment"># calc y</span></span><br><span class="line">    y = b</span><br><span class="line">    <span class="comment"># print(len(a))</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(a)):</span><br><span class="line">        y += w[i]*a[i] + w[i+<span class="number">9</span>]*(a[i]**<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">LF</span><span class="params">()</span>:</span> <span class="comment"># loss function</span></span><br><span class="line">    tot1, tot2 = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">9</span>, len(data[<span class="number">9</span>])<span class="number">-1</span>):</span><br><span class="line">        tot1 += data[<span class="number">9</span>][i]-f(data[<span class="number">9</span>][i<span class="number">-9</span>: i])</span><br><span class="line">        tot2 += (data[<span class="number">9</span>][i]-f(data[<span class="number">9</span>][i<span class="number">-9</span>: i]))**<span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> tot1, tot2</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getpart</span><span class="params">()</span>:</span> <span class="comment"># get partial w_i,b</span></span><br><span class="line">    <span class="keyword">global</span> w, b, learningrate, g, gb</span><br><span class="line">    parw = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">18</span>)]</span><br><span class="line">    parb = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">9</span>, len(data[<span class="number">9</span>])<span class="number">-1</span>):</span><br><span class="line">        a = data[<span class="number">9</span>][i<span class="number">-9</span>: i]</span><br><span class="line">        tmp = data[<span class="number">9</span>][i]-f(a)</span><br><span class="line">        <span class="comment"># print('a = ', a, ' tmp = ', tmp, 'f(a) = ', f(a))</span></span><br><span class="line">        parb += <span class="number">-2</span>*(tmp)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">9</span>):</span><br><span class="line">            parw[j] += <span class="number">-2</span>*tmp*a[j]</span><br><span class="line">            parw[j+<span class="number">9</span>] += <span class="number">-2</span>*tmp*a[j]*a[j]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">18</span>):</span><br><span class="line">        g[i] += parw[i]**<span class="number">2</span></span><br><span class="line">    gb += parb**<span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># print(parw, parb)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">18</span>):</span><br><span class="line">        w[i]=w[i]-learningrate/np.sqrt(g[i])*parw[i]</span><br><span class="line">    b = b-learningrate/np.sqrt(gb)*parb</span><br><span class="line"></span><br><span class="line"><span class="comment"># main loop</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">times = <span class="number">10000</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(times): </span><br><span class="line">    loss1, loss2 = LF()</span><br><span class="line">    print(loss2)</span><br><span class="line">    getpart()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(w, b)</span><br><span class="line">print(<span class="string">'----------------------'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># read test data &amp; output result</span></span><br><span class="line">ff = open(<span class="string">'test.csv'</span>, <span class="string">'r'</span>, <span class="keyword">True</span>, <span class="string">'big5hkscs'</span>)</span><br><span class="line">fo = open(<span class="string">'ans.csv'</span>, <span class="string">'w'</span>, <span class="keyword">True</span>, <span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line">dd = ff.readlines()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'id,value'</span>, file=fo)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> dd:</span><br><span class="line">    line = i.split(<span class="string">','</span>)</span><br><span class="line">    <span class="comment"># print(line)</span></span><br><span class="line">    <span class="keyword">if</span>(line[<span class="number">1</span>].startswith(<span class="string">"PM2.5"</span>)):</span><br><span class="line">        a = []</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> line[<span class="number">2</span>:]:</span><br><span class="line">            a.append(float(j))</span><br><span class="line">        print(line[<span class="number">0</span>], <span class="string">','</span>, f(a), file=fo, sep=<span class="string">''</span>)</span><br></pre></td></tr></table></figure>
<p>加了三次方，感觉要Overfitting…<br>哦 结果变得肥肠爆炸…</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># read data</span></span><br><span class="line"></span><br><span class="line">ff = open(<span class="string">'train.csv'</span>, <span class="string">'r'</span>, <span class="keyword">True</span>, <span class="string">'big5hkscs'</span>);</span><br><span class="line">data_init = ff.readlines()</span><br><span class="line">total_line = len(data_init)<span class="number">-1</span></span><br><span class="line"><span class="comment"># total_line = 19</span></span><br><span class="line">total_type = <span class="number">18</span></span><br><span class="line"></span><br><span class="line">data = [[] <span class="keyword">for</span> i <span class="keyword">in</span> range(total_type)]</span><br><span class="line"><span class="comment"># print(data)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, total_line+<span class="number">1</span>): </span><br><span class="line">    <span class="comment"># print(i, data_init[i], end=' ')</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> data_init[i].split(<span class="string">','</span>)[<span class="number">3</span>:]: </span><br><span class="line">        <span class="comment"># print(j, end=' ')</span></span><br><span class="line">        <span class="keyword">if</span>(j.startswith(<span class="string">"NR"</span>)):</span><br><span class="line">            data[(i<span class="number">-1</span>)%total_type].append(<span class="number">0.0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            data[(i<span class="number">-1</span>)%total_type].append(float(j))</span><br><span class="line">    <span class="comment"># print()</span></span><br><span class="line"><span class="comment"># print(data)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># for i in data:</span></span><br><span class="line"><span class="comment">#     print(len(i))</span></span><br><span class="line"></span><br><span class="line">ff.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># init model without adagrad</span></span><br><span class="line"><span class="comment"># just consider one type</span></span><br><span class="line"></span><br><span class="line">learningrate = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">w = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">27</span>)]</span><br><span class="line">b = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">g = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">27</span>)]</span><br><span class="line">gb = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># print(data[9])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(a)</span>:</span> <span class="comment"># calc y</span></span><br><span class="line">    y = b</span><br><span class="line">    <span class="comment"># print(len(a))</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">9</span>):</span><br><span class="line">        y += w[i]*a[i] + w[i+<span class="number">9</span>]*(a[i]**<span class="number">2</span>) + w[i+<span class="number">18</span>]*(a[i]**<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">LF</span><span class="params">()</span>:</span> <span class="comment"># loss function</span></span><br><span class="line">    tot1, tot2 = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">9</span>, len(data[<span class="number">9</span>])<span class="number">-1</span>):</span><br><span class="line">        tot1 += data[<span class="number">9</span>][i]-f(data[<span class="number">9</span>][i<span class="number">-9</span>: i])</span><br><span class="line">        tot2 += (data[<span class="number">9</span>][i]-f(data[<span class="number">9</span>][i<span class="number">-9</span>: i]))**<span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> tot1, tot2</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getpart</span><span class="params">()</span>:</span> <span class="comment"># get partial w_i,b</span></span><br><span class="line">    <span class="keyword">global</span> w, b, learningrate, g, gb</span><br><span class="line">    parw = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">27</span>)]</span><br><span class="line">    parb = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">9</span>, len(data[<span class="number">9</span>])<span class="number">-1</span>):</span><br><span class="line">        a = data[<span class="number">9</span>][i<span class="number">-9</span>: i]</span><br><span class="line">        tmp = data[<span class="number">9</span>][i]-f(a)</span><br><span class="line">        <span class="comment"># print('a = ', a, ' tmp = ', tmp, 'f(a) = ', f(a))</span></span><br><span class="line">        parb += <span class="number">-2</span>*(tmp)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">9</span>):</span><br><span class="line">            parw[j] += <span class="number">-2</span>*tmp*a[j]</span><br><span class="line">            parw[j+<span class="number">9</span>] += <span class="number">-2</span>*tmp*a[j]*a[j]</span><br><span class="line">            parw[j+<span class="number">18</span>] += <span class="number">-2</span>*tmp*(a[j]**<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">27</span>):</span><br><span class="line">        g[i] += parw[i]**<span class="number">2</span></span><br><span class="line">    gb += parb**<span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># print(parw, parb)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">27</span>):</span><br><span class="line">        w[i]=w[i]-learningrate/np.sqrt(g[i])*parw[i]</span><br><span class="line">    b = b-learningrate/np.sqrt(gb)*parb</span><br><span class="line"></span><br><span class="line"><span class="comment"># main loop</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">times = <span class="number">10000</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(times): </span><br><span class="line">    loss1, loss2 = LF()</span><br><span class="line">    print(loss2)</span><br><span class="line">    getpart()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(w, b)</span><br><span class="line">print(<span class="string">'----------------------'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># read test data &amp; output result</span></span><br><span class="line">ff = open(<span class="string">'test.csv'</span>, <span class="string">'r'</span>, <span class="keyword">True</span>, <span class="string">'big5hkscs'</span>)</span><br><span class="line">fo = open(<span class="string">'ans.csv'</span>, <span class="string">'w'</span>, <span class="keyword">True</span>, <span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line">dd = ff.readlines()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'id,value'</span>, file=fo)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> dd:</span><br><span class="line">    line = i.split(<span class="string">','</span>)</span><br><span class="line">    <span class="comment"># print(line)</span></span><br><span class="line">    <span class="keyword">if</span>(line[<span class="number">1</span>].startswith(<span class="string">"PM2.5"</span>)):</span><br><span class="line">        a = []</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> line[<span class="number">2</span>:]:</span><br><span class="line">            a.append(float(j))</span><br><span class="line">        print(line[<span class="number">0</span>], <span class="string">','</span>, f(a), file=fo, sep=<span class="string">''</span>)</span><br></pre></td></tr></table></figure>
<h2 id="hw2"><a href="#hw2" class="headerlink" title="hw2"></a>hw2</h2><p><a href="https://www.kaggle.com/c/ml2019spring-hw2" target="_blank" rel="noopener">https://www.kaggle.com/c/ml2019spring-hw2</a><br><a href="https://ntumlta2019.github.io/ml-web-hw2/" target="_blank" rel="noopener">作业说明</a></p>
<p>这就是一个Binary Classification，直接用X_train提出的特征就可以了</p>
<h3 id="Probabilistic-Generateive-Model"><a href="#Probabilistic-Generateive-Model" class="headerlink" title="Probabilistic Generateive Model"></a>Probabilistic Generateive Model</h3><p>假设是高斯分布来算的，所以不需要训练，秒出结果，得分也不太高<code>private:0.76047 public:0.76707</code><br>也不知道是不是我写挂了…</p>
<p>没错是我写挂了，这全是0吧…</p>
<p>ps.手写矩阵运算好麻烦啊，虽然也不长，但是种类太多，感觉不如C++方便，可能我都写C++写惯了，抽时间看一下numpy好了</p>
<p>probabilistic_generateive_model.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> pi, log, exp</span><br><span class="line"></span><br><span class="line">file_x = open(<span class="string">'X_train'</span>, <span class="string">'r'</span>)</span><br><span class="line">file_y = open(<span class="string">'Y_train'</span>, <span class="string">'r'</span>)</span><br><span class="line">file_xt = open(<span class="string">'X_test'</span>, <span class="string">'r'</span>)</span><br><span class="line">file_yt = open(<span class="string">'Y_test'</span>, <span class="string">'w'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#x_train = y_train = x_test = []</span></span><br><span class="line"><span class="comment">#mu1 = mu2 = sigm = []</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">deal_data</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(x)):</span><br><span class="line">        x[i]=x[i].split(<span class="string">','</span>)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, len(x[i])):</span><br><span class="line">            x[i][j]=int(x[i][j])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">global</span> x_train, y_train, x_test</span><br><span class="line">    x_train = file_x.readlines()[<span class="number">1</span>:]</span><br><span class="line">    y_train = file_y.readlines()[<span class="number">1</span>:]</span><br><span class="line">    x_test = file_xt.readlines()[<span class="number">1</span>:]</span><br><span class="line">    </span><br><span class="line">    deal_data(x_train)</span><br><span class="line">    deal_data(y_train)</span><br><span class="line">    deal_data(x_test)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mult</span><span class="params">(a, b)</span>:</span></span><br><span class="line">    c = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(a)):</span><br><span class="line">        c.append([])</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(b[<span class="number">0</span>])):</span><br><span class="line">            c[i].append(<span class="number">0</span>)</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(a)):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(b)):</span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> range(len(b[<span class="number">0</span>])):</span><br><span class="line">                c[i][k] += a[i][j]*b[j][k]</span><br><span class="line">    <span class="keyword">return</span> c</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mult_num</span><span class="params">(a, b)</span>:</span></span><br><span class="line">    c = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(a)):</span><br><span class="line">        c.append([])</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(a[i])):</span><br><span class="line">            c[i].append(a[i][j]*b)</span><br><span class="line">    <span class="keyword">return</span> c</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(a, b)</span>:</span></span><br><span class="line">    c = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(a)):</span><br><span class="line">        c.append([])</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(a[<span class="number">0</span>])):</span><br><span class="line">            c[i].append(<span class="number">0</span>)</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(a)):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(a[<span class="number">0</span>])):</span><br><span class="line">            c[i][j] = a[i][j]+b[i][j]</span><br><span class="line">    <span class="keyword">return</span> c</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transpose</span><span class="params">(a)</span>:</span></span><br><span class="line">    c = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(a[<span class="number">0</span>])):</span><br><span class="line">        c.append([])</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(a)):</span><br><span class="line">            c[i].append(a[j][i])</span><br><span class="line">    <span class="keyword">return</span> c </span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">global</span> x_train, y_train, x_test</span><br><span class="line">    <span class="keyword">global</span> n, n0, n1, dim, mu0, mu1, sigm, sigm0, sigm1, w, b</span><br><span class="line">    n = len(y_train)</span><br><span class="line">    n0 = n1 = <span class="number">0</span></span><br><span class="line">    dim = len(x_train[<span class="number">0</span>])</span><br><span class="line">    mu0 = []</span><br><span class="line">    mu1 = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(dim):</span><br><span class="line">        mu0.append(<span class="number">0</span>)</span><br><span class="line">        mu1.append(<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        <span class="keyword">if</span> y_train[i][<span class="number">0</span>] == <span class="number">0</span>:</span><br><span class="line">            n0+=<span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(dim):</span><br><span class="line">                mu0[j] += x_train[i][j]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            n1+=<span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(dim):</span><br><span class="line">                mu1[j] += x_train[i][j]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(dim):</span><br><span class="line">        mu0[i] /= n0</span><br><span class="line">        mu1[i] /= n1</span><br><span class="line">    </span><br><span class="line">    sigm0 = <span class="number">0</span></span><br><span class="line">    sigm1 = <span class="number">0</span></span><br><span class="line"><span class="comment">#    for i in range(dim):</span></span><br><span class="line"><span class="comment">#        sigm0.append(0)</span></span><br><span class="line"><span class="comment">#        sigm1.append(0)</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        <span class="keyword">if</span> y_train[i][<span class="number">0</span>] == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(dim):</span><br><span class="line">                sigm0+=(x_train[i][j]-mu0[j])**<span class="number">2</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(dim):</span><br><span class="line">                sigm1+=(x_train[i][j]-mu1[j])**<span class="number">2</span></span><br><span class="line">    </span><br><span class="line">    sigm0 /= n0</span><br><span class="line">    sigm1 /= n1</span><br><span class="line">    sigm = n0/(n0+n1)*sigm0 + n1/(n0+n1)*sigm1</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#change to vector</span></span><br><span class="line">        </span><br><span class="line">    w = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(dim):</span><br><span class="line">        w.append((mu0[i]-mu1[i])/sigm)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(dim):</span><br><span class="line">        mu0[i] = [mu0[i]]</span><br><span class="line">        mu1[i] = [mu1[i]]</span><br><span class="line">    </span><br><span class="line">    b0 = mult_num(mult(transpose(mu0), mu0), <span class="number">-0.5</span>/sigm)</span><br><span class="line">    b1 = mult_num(mult(transpose(mu1), mu1), <span class="number">0.5</span>/sigm)</span><br><span class="line">    b = add(b0, b1)</span><br><span class="line">    b = b[<span class="number">0</span>][<span class="number">0</span>]+log(n0/n1)</span><br><span class="line">    </span><br><span class="line"><span class="comment">#    print(b)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#    for i in range(dim):</span></span><br><span class="line"><span class="comment">#        sigm0[i] /= n0</span></span><br><span class="line"><span class="comment">#        sigm1[i] /= n1</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+exp(-z))</span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_probability</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> w, b</span><br><span class="line">    z=<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(dim):</span><br><span class="line">       z += w[i]*x[i]</span><br><span class="line">    z += b</span><br><span class="line">    <span class="keyword">return</span> sigmoid(z)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_predict</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> get_probability(x)&gt;<span class="number">0.5</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">'id,label'</span>, file=file_yt)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(x_test)):</span><br><span class="line">        print(i+<span class="number">1</span>, get_predict(x_test[i]),sep=<span class="string">','</span>, file=file_yt)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span>    </span><br><span class="line">    load_data()</span><br><span class="line">    print(<span class="string">'load_data() ok'</span>)</span><br><span class="line">    train()</span><br><span class="line">    print(<span class="string">'train() ok'</span>)</span><br><span class="line">    test()</span><br><span class="line">    print(<span class="string">'test() ok'</span>)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>: </span><br><span class="line">    main()</span><br></pre></td></tr></table></figure></p>
<h3 id="Logistic-Regression-Model"><a href="#Logistic-Regression-Model" class="headerlink" title="Logistic Regression Model"></a>Logistic Regression Model</h3><p>设$s(x)=\frac{1}{1+e^{-x}}$，那么$s’(x)=s(x)(1-s(x))$，反过来证明很简单，正推求导还是有些麻烦的<br>这个1w数据，又有很多浮点运算…实在是难顶…好慢…</p>
<p>哎，写了个Logistic Regression，train来train去，0还是太少了，我佛啦</p>
<p>我发现它每个几个就会出现一次结果非常糟的情况…</p>
<p>参考了下别人的代码魔改了一发，就是加上了AdaGrad，然后求偏微分的时候除总数，再加上Regularization</p>
<p>哇！ 我懂了！<br>有个肥肠肥肠肥肠肥肠肥肠肥肠肥肠肥肠肥肠重要的数据处理就是把一些范围肥肠大的数据范围变小，就除均值就可以了，在这里，有第0,1,3,4,5有关年龄，收入支出的部分范围特别大，把他范围缩小，然后再train就不会出现之前那样每个几个就会出现一次结果非常糟的情况了，train 30次就能得到<code>private:0.84363 public:0.84656</code></p>
<p>最高也就<code>private: 0.85345 public:0.85356</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">file_x = open(<span class="string">'X_train'</span>, <span class="string">'r'</span>)</span><br><span class="line">file_y = open(<span class="string">'Y_train'</span>, <span class="string">'r'</span>)</span><br><span class="line">file_xt = open(<span class="string">'X_test'</span>, <span class="string">'r'</span>)</span><br><span class="line">file_yt = open(<span class="string">'Y_test'</span>, <span class="string">'w'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#x_train = y_train = x_test = []</span></span><br><span class="line"><span class="comment">#mu1 = mu2 = sigm = []</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">deal_data</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(x)):</span><br><span class="line">        x[i]=x[i].split(<span class="string">','</span>)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, len(x[i])):</span><br><span class="line">            x[i][j]=int(x[i][j])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">deal_col</span><span class="params">(x, a, xt)</span>:</span></span><br><span class="line">    mean_x = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(x)):</span><br><span class="line">        mean_x += x[i][a]</span><br><span class="line">    mean_x /= len(x)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(x)):</span><br><span class="line">        x[i][a] /= mean_x</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(xt)):</span><br><span class="line">        xt[i][a] /= mean_x</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">global</span> x_train, y_train, x_test, x_val, y_val</span><br><span class="line">    x_train = file_x.readlines()[<span class="number">1</span>:]</span><br><span class="line">    y_train = file_y.readlines()[<span class="number">1</span>:]</span><br><span class="line">    x_test = file_xt.readlines()[<span class="number">1</span>:]</span><br><span class="line">    </span><br><span class="line">    deal_data(x_train)</span><br><span class="line">    deal_data(y_train)</span><br><span class="line">    deal_data(x_test)</span><br><span class="line">    </span><br><span class="line">    deal_col(x_train, <span class="number">0</span>, x_test)</span><br><span class="line">    deal_col(x_train, <span class="number">1</span>, x_test)</span><br><span class="line">    deal_col(x_train, <span class="number">3</span>, x_test)</span><br><span class="line">    deal_col(x_train, <span class="number">4</span>, x_test)</span><br><span class="line">    deal_col(x_train, <span class="number">5</span>, x_test)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    x = x_train</span><br><span class="line">    y = y_train</span><br><span class="line">    x_train, x_val = x[:<span class="number">20000</span>], x[<span class="number">20000</span>:]</span><br><span class="line">    y_train, y_val = y[:<span class="number">20000</span>], y[<span class="number">20000</span>:]</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+np.exp(-z))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_loss</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> w, b, dim</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(dim):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">global</span> x_train, y_train, x_test, x_val, y_val</span><br><span class="line">    <span class="keyword">global</span> w, b, dim</span><br><span class="line">    w = []</span><br><span class="line">    dim = len(x_train[<span class="number">0</span>])</span><br><span class="line">    b = <span class="number">0</span></span><br><span class="line">    lr = <span class="number">1</span></span><br><span class="line">    y = []</span><br><span class="line">    reg_rate = <span class="number">0.001</span></span><br><span class="line">    </span><br><span class="line">    gradient_w_sum = []</span><br><span class="line">    gradient_b_sum = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(dim):</span><br><span class="line">        w.append(<span class="number">1</span>)</span><br><span class="line">        gradient_w_sum.append(<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">200</span>):</span><br><span class="line">        print(t, <span class="string">' testing...'</span>, end=<span class="string">''</span>)</span><br><span class="line">        y = []</span><br><span class="line">        err = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(x_train)):</span><br><span class="line">            y.append(b)</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(dim):</span><br><span class="line">                y[i] += x_train[i][j]*w[j]</span><br><span class="line">            y[i] = sigmoid(y[i])</span><br><span class="line">            <span class="keyword">if</span> (y[i]&gt;<span class="number">0.5</span>) != y_train[i][<span class="number">0</span>]:</span><br><span class="line">                err += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        print(<span class="string">' train acc:'</span>, <span class="number">1</span>-err/len(x_train), end=<span class="string">''</span>)</span><br><span class="line">        </span><br><span class="line">        err = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(x_val)):</span><br><span class="line">            yt = b</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(dim):</span><br><span class="line">                yt += x_val[i][j]*w[j]</span><br><span class="line">            yt = sigmoid(yt)</span><br><span class="line">            <span class="keyword">if</span> (yt&gt;<span class="number">0.5</span>) != y_val[i][<span class="number">0</span>]:</span><br><span class="line">                err += <span class="number">1</span></span><br><span class="line">        print(<span class="string">' val acc:'</span>, <span class="number">1</span>-err/len(x_val))</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(dim):</span><br><span class="line">            gradient_w = <span class="number">2</span>*reg_rate*w[i]</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(len(x_train)):</span><br><span class="line">                gradient_w += -(y_train[j][<span class="number">0</span>]-y[j])*x_train[j][i]</span><br><span class="line">            gradient_w = gradient_w/len(x_train)</span><br><span class="line">            gradient_w_sum[i] += gradient_w**<span class="number">2</span></span><br><span class="line">            w[i] = w[i] - lr/np.sqrt(gradient_w_sum[i])*gradient_w</span><br><span class="line"></span><br><span class="line">        gradient_b = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(x_train)):</span><br><span class="line">            gradient_b += -(y_train[i][<span class="number">0</span>]-y[i])</span><br><span class="line">            </span><br><span class="line">        gradient_b = gradient_b/len(x_train)</span><br><span class="line">        gradient_b_sum += gradient_b**<span class="number">2</span></span><br><span class="line">        b = b - lr/np.sqrt(gradient_b_sum)*gradient_b</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_probability</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> w, b, dim</span><br><span class="line">    z = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(dim):</span><br><span class="line">       z += w[i]*x[i]</span><br><span class="line">    z += b</span><br><span class="line">    <span class="keyword">return</span> sigmoid(z)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_predict</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="comment"># print(get_probability(x))</span></span><br><span class="line">    <span class="keyword">if</span> get_probability(x)&gt;<span class="number">0.5</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">'id,label'</span>, file=file_yt)</span><br><span class="line">    nn = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(x_test)):</span><br><span class="line">        <span class="keyword">if</span> get_predict(x_test[i])==<span class="number">0</span>:</span><br><span class="line">            nn += <span class="number">1</span></span><br><span class="line">        print(i+<span class="number">1</span>, get_predict(x_test[i]),sep=<span class="string">','</span>, file=file_yt)</span><br><span class="line">    print(nn, len(x_test))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    load_data()</span><br><span class="line">    print(<span class="string">'load_data() ok'</span>)</span><br><span class="line">    train()</span><br><span class="line">    print(<span class="string">'train() ok'</span>)</span><br><span class="line">    print(w, b)</span><br><span class="line">    test()</span><br><span class="line">    print(<span class="string">'test() ok'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#main</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<p>我又修改了一下处理数据那部分，把范围大的变成$\frac{x-\min\{x\}}{\max\{x\}-\min\{x\}}$<br>结果稍差了一丢丢<code>private:0.84903 public:0.85282</code><br>不管了…看后面的了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">deal_col</span><span class="params">(x, a, xt)</span>:</span></span><br><span class="line">    mxx = x[<span class="number">0</span>][a]</span><br><span class="line">    mix = x[<span class="number">0</span>][a]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(x)):</span><br><span class="line">        mxx = max(mxx, x[i][a])</span><br><span class="line">        mix = min(mix, x[i][a])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(x)):</span><br><span class="line">        x[i][a] = (x[i][a]-mix)/(mxx-mix)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(xt)):</span><br><span class="line">        xt[i][a] = (xt[i][a]-mix)/(mxx-mix)</span><br></pre></td></tr></table></figure>
<h2 id="hw3"><a href="#hw3" class="headerlink" title="hw3"></a>hw3</h2><p><a href="https://www.kaggle.com/c/ml2019spring-hw3" target="_blank" rel="noopener">https://www.kaggle.com/c/ml2019spring-hw3</a><br><a href="https://ntumlta2019.github.io/ml-web-hw3/" target="_blank" rel="noopener">作业说明</a></p>
<h3 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h3><p>要做的是一个人脸表情的分类<br>数据处理的部分用了好长时间，主要是显式循环太致命了…我还不太会处理csv，所以直接暴力读文件处理了<br>这是最一开始写的，得到的output是这样<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">Epoch 1/20</span><br><span class="line">2019-07-30 23:58:40.386315: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA</span><br><span class="line">2019-07-30 23:58:40.410366: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2494135000 Hz</span><br><span class="line">2019-07-30 23:58:40.411071: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5616a3e35300 executing computations on platform Host. Devices:</span><br><span class="line">2019-07-30 23:58:40.411112: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;</span><br><span class="line">2019-07-30 23:58:40.587853: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.</span><br><span class="line">28709/28709 [==============================] - 21s 724us/step - loss: 1.6676 - acc: 0.3480</span><br><span class="line">Epoch 2/20</span><br><span class="line">28709/28709 [==============================] - 21s 715us/step - loss: 1.5076 - acc: 0.4226</span><br><span class="line">Epoch 3/20</span><br><span class="line">28709/28709 [==============================] - 21s 748us/step - loss: 1.3983 - acc: 0.4698</span><br><span class="line">Epoch 4/20</span><br><span class="line">28709/28709 [==============================] - 21s 737us/step - loss: 1.2961 - acc: 0.5130</span><br><span class="line">Epoch 5/20</span><br><span class="line">28709/28709 [==============================] - 21s 742us/step - loss: 1.1895 - acc: 0.5582</span><br><span class="line">Epoch 6/20</span><br><span class="line">28709/28709 [==============================] - 21s 739us/step - loss: 1.0732 - acc: 0.6067</span><br><span class="line">Epoch 7/20</span><br><span class="line">28709/28709 [==============================] - 22s 772us/step - loss: 0.9517 - acc: 0.6570</span><br><span class="line">Epoch 8/20</span><br><span class="line">28709/28709 [==============================] - 23s 797us/step - loss: 0.8311 - acc: 0.7044</span><br><span class="line">Epoch 9/20</span><br><span class="line">28709/28709 [==============================] - 23s 792us/step - loss: 0.7124 - acc: 0.7521</span><br><span class="line">Epoch 10/20</span><br><span class="line">28709/28709 [==============================] - 22s 769us/step - loss: 0.6000 - acc: 0.7962</span><br><span class="line">Epoch 11/20</span><br><span class="line">28709/28709 [==============================] - 22s 759us/step - loss: 0.5022 - acc: 0.8347</span><br><span class="line">Epoch 12/20</span><br><span class="line">28709/28709 [==============================] - 23s 793us/step - loss: 0.4063 - acc: 0.8699</span><br><span class="line">Epoch 13/20</span><br><span class="line">28709/28709 [==============================] - 23s 800us/step - loss: 0.3259 - acc: 0.9009</span><br><span class="line">Epoch 14/20</span><br><span class="line">28709/28709 [==============================] - 23s 793us/step - loss: 0.2617 - acc: 0.9235</span><br><span class="line">Epoch 15/20</span><br><span class="line">28709/28709 [==============================] - 23s 790us/step - loss: 0.1984 - acc: 0.9473</span><br><span class="line">Epoch 16/20</span><br><span class="line">28709/28709 [==============================] - 23s 801us/step - loss: 0.1556 - acc: 0.9639</span><br><span class="line">Epoch 17/20</span><br><span class="line">28709/28709 [==============================] - 23s 789us/step - loss: 0.1387 - acc: 0.9668</span><br><span class="line">Epoch 18/20</span><br><span class="line">28709/28709 [==============================] - 22s 768us/step - loss: 0.1521 - acc: 0.9577</span><br><span class="line">Epoch 19/20</span><br><span class="line">28709/28709 [==============================] - 23s 818us/step - loss: 0.0914 - acc: 0.9804</span><br><span class="line">Epoch 20/20</span><br><span class="line">28709/28709 [==============================] - 22s 755us/step - loss: 0.0693 - acc: 0.9884</span><br><span class="line">28709/28709 [==============================] - 11s 392us/step</span><br><span class="line"></span><br><span class="line">Train Acc: 0.9921975671163343</span><br></pre></td></tr></table></figure></p>
<p>提交得到的分数很低<code>private:0.47394 public:0.49456</code><br>我觉得是batch_size和epochs没设置好，明天调<br>有一个地方就是Conv2D的Dimension一开始实在是没搞懂，好像必须要3维才可以，然后就搞成我写的这样可以成功运行</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers.core <span class="keyword">import</span> Dense, Dropout, Activation</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Conv2D, MaxPooling2D, Flatten</span><br><span class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> SGD, Adam</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">()</span>:</span></span><br><span class="line">    train_f = open(<span class="string">'train.csv'</span>, <span class="string">'r'</span>)</span><br><span class="line">    test_f = open(<span class="string">'test.csv'</span>, <span class="string">'r'</span>)</span><br><span class="line">    train_data = train_f.readlines()[<span class="number">1</span>:]</span><br><span class="line">    test_data = test_f.readlines()[<span class="number">1</span>:]</span><br><span class="line">    </span><br><span class="line">    x_train = []</span><br><span class="line">    y_train = []</span><br><span class="line">    x_test = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(train_data)):</span><br><span class="line">        train_data[i]=train_data[i].split(<span class="string">','</span>)</span><br><span class="line">        x_train.append(train_data[i][<span class="number">1</span>].split(<span class="string">' '</span>))</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(x_train[i])):</span><br><span class="line">            x_train[i][j]=int(x_train[i][j])</span><br><span class="line">        y_train.append(int(train_data[i][<span class="number">0</span>]))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(test_data)):</span><br><span class="line">        test_data[i]=test_data[i].split(<span class="string">','</span>)</span><br><span class="line">        x_test.append(test_data[i][<span class="number">1</span>].split(<span class="string">' '</span>))</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(x_test[i])):</span><br><span class="line">            x_test[i][j]=int(x_test[i][j])</span><br><span class="line">    </span><br><span class="line">    x = np.array(x_train)</span><br><span class="line">    y = np.array(y_train)</span><br><span class="line">    xx = np.array(x_test)</span><br><span class="line">    </span><br><span class="line">    x_train = x</span><br><span class="line">    y_train = y</span><br><span class="line">    x_test = xx</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># deal data</span></span><br><span class="line">    total_data = x_train.shape[<span class="number">0</span>]</span><br><span class="line">    x_train = x_train.reshape(total_data, <span class="number">48</span>, <span class="number">48</span>, <span class="number">1</span>)</span><br><span class="line">    x_test = x_test.reshape(x_test.shape[<span class="number">0</span>], <span class="number">48</span>, <span class="number">48</span>, <span class="number">1</span>)</span><br><span class="line">    x_train = x_train.astype(<span class="string">'float32'</span>)</span><br><span class="line">    x_test = x_test.astype(<span class="string">'float32'</span>)</span><br><span class="line">    x_train = x_train / <span class="number">255</span></span><br><span class="line">    x_test = x_test / <span class="number">255</span></span><br><span class="line">    </span><br><span class="line">    y_train = np_utils.to_categorical(y_train, <span class="number">7</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> x_train, y_train, x_test</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(x_train, y_train)</span>:</span></span><br><span class="line">    model = Sequential()</span><br><span class="line">    model.add(Conv2D(<span class="number">25</span>, (<span class="number">6</span>, <span class="number">6</span>), input_shape=(x_train.shape[<span class="number">1</span>], x_train.shape[<span class="number">2</span>], x_train.shape[<span class="number">3</span>])))</span><br><span class="line">    model.add(MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">    model.add(Flatten())</span><br><span class="line">    model.add(Dense(output_dim = <span class="number">100</span>))</span><br><span class="line">    model.add(Activation(<span class="string">'relu'</span>))</span><br><span class="line">    model.add(Dense(output_dim = <span class="number">7</span>))</span><br><span class="line">    model.add(Activation(<span class="string">'softmax'</span>))</span><br><span class="line">    model.compile(loss=<span class="string">'categorical_crossentropy'</span>, optimizer=<span class="string">'adam'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">    model.fit(x_train, y_train, batch_size = <span class="number">100</span>, epochs = <span class="number">20</span>)</span><br><span class="line">    result = model.evaluate(x_train, y_train, batch_size=<span class="number">10</span>)</span><br><span class="line">    print(<span class="string">'\nTrain Acc:'</span>, result[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(model, x_test)</span>:</span></span><br><span class="line">    y_test = model.predict(x_test)</span><br><span class="line">    test_f = open(<span class="string">'y_test.csv'</span>, <span class="string">'w'</span>)</span><br><span class="line">    print(<span class="string">'id,label'</span>, file=test_f)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(y_test)):</span><br><span class="line">        print(i, np.argmax(y_test[i]), sep=<span class="string">','</span>, file=test_f)</span><br><span class="line">    test_f.close()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    x_train, y_train, x_test = load_data()</span><br><span class="line">    print(<span class="string">'load_data() ok'</span>)</span><br><span class="line">    model = train(x_train, y_train)</span><br><span class="line">    print(<span class="string">'train() ok'</span>)</span><br><span class="line">    test(model, x_test)</span><br><span class="line">    print(<span class="string">'test() ok'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<p>后来我改了一下，得分<code>private: 0.45500 public:0.45583</code>，还是不太行嗷，回头改一下Model，我先睡了zzz<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(x_train, y_train)</span>:</span></span><br><span class="line">    model = Sequential()</span><br><span class="line">    model.add(Conv2D(<span class="number">25</span>, (<span class="number">6</span>, <span class="number">6</span>), input_shape=(x_train.shape[<span class="number">1</span>], x_train.shape[<span class="number">2</span>], x_train.shape[<span class="number">3</span>])))</span><br><span class="line">    model.add(MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">    model.add(Flatten())</span><br><span class="line">    model.add(Dense(output_dim = <span class="number">100</span>))</span><br><span class="line">    model.add(Activation(<span class="string">'relu'</span>))</span><br><span class="line">    model.add(Dense(output_dim = <span class="number">7</span>))</span><br><span class="line">    model.add(Activation(<span class="string">'softmax'</span>))</span><br><span class="line">    model.compile(loss=<span class="string">'categorical_crossentropy'</span>, optimizer=<span class="string">'adam'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">    model.fit(x_train, y_train, batch_size = <span class="number">10</span>, epochs = <span class="number">20</span>)</span><br><span class="line">    result = model.evaluate(x_train, y_train, batch_size=<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">'\nTrain Acc:'</span>, result[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure></p>
<p>output<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">Epoch 1/20</span><br><span class="line">2019-07-31 00:08:55.510380: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA</span><br><span class="line">2019-07-31 00:08:55.534347: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2494135000 Hz</span><br><span class="line">2019-07-31 00:08:55.535015: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5639b5442300 executing computations on platform Host. Devices:</span><br><span class="line">2019-07-31 00:08:55.535039: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;</span><br><span class="line">2019-07-31 00:08:55.662054: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.</span><br><span class="line">28709/28709 [==============================] - 75s 3ms/step - loss: 1.6555 - acc: 0.3503</span><br><span class="line">Epoch 2/20</span><br><span class="line">28709/28709 [==============================] - 85s 3ms/step - loss: 1.5076 - acc: 0.4157</span><br><span class="line">Epoch 3/20</span><br><span class="line">28709/28709 [==============================] - 83s 3ms/step - loss: 1.4020 - acc: 0.4644</span><br><span class="line">Epoch 4/20</span><br><span class="line">28709/28709 [==============================] - 84s 3ms/step - loss: 1.3111 - acc: 0.5012</span><br><span class="line">Epoch 5/20</span><br><span class="line">28709/28709 [==============================] - 81s 3ms/step - loss: 1.2128 - acc: 0.5410</span><br><span class="line">Epoch 6/20</span><br><span class="line">28709/28709 [==============================] - 83s 3ms/step - loss: 1.1142 - acc: 0.5840</span><br><span class="line">Epoch 7/20</span><br><span class="line">28709/28709 [==============================] - 82s 3ms/step - loss: 1.0072 - acc: 0.6264</span><br><span class="line">Epoch 8/20</span><br><span class="line">28709/28709 [==============================] - 82s 3ms/step - loss: 0.9050 - acc: 0.6643</span><br><span class="line">Epoch 9/20</span><br><span class="line">28709/28709 [==============================] - 81s 3ms/step - loss: 0.8023 - acc: 0.7046</span><br><span class="line">Epoch 10/20</span><br><span class="line">28709/28709 [==============================] - 81s 3ms/step - loss: 0.7115 - acc: 0.7392</span><br><span class="line">Epoch 11/20</span><br><span class="line">28709/28709 [==============================] - 77s 3ms/step - loss: 0.6215 - acc: 0.7720</span><br><span class="line">Epoch 12/20</span><br><span class="line">28709/28709 [==============================] - 74s 3ms/step - loss: 0.5508 - acc: 0.7971</span><br><span class="line">Epoch 13/20</span><br><span class="line">28709/28709 [==============================] - 76s 3ms/step - loss: 0.4865 - acc: 0.8204</span><br><span class="line">Epoch 14/20</span><br><span class="line">28709/28709 [==============================] - 77s 3ms/step - loss: 0.4371 - acc: 0.8413</span><br><span class="line">Epoch 15/20</span><br><span class="line">28709/28709 [==============================] - 78s 3ms/step - loss: 0.3915 - acc: 0.8592</span><br><span class="line">Epoch 16/20</span><br><span class="line">28709/28709 [==============================] - 76s 3ms/step - loss: 0.3626 - acc: 0.8690</span><br><span class="line">Epoch 17/20</span><br><span class="line">28709/28709 [==============================] - 76s 3ms/step - loss: 0.3190 - acc: 0.8858</span><br><span class="line">Epoch 18/20</span><br><span class="line">28709/28709 [==============================] - 76s 3ms/step - loss: 0.3013 - acc: 0.8930</span><br><span class="line">Epoch 19/20</span><br><span class="line">28709/28709 [==============================] - 76s 3ms/step - loss: 0.2744 - acc: 0.9027</span><br><span class="line">Epoch 20/20</span><br><span class="line">28709/28709 [==============================] - 77s 3ms/step - loss: 0.2455 - acc: 0.9124</span><br><span class="line">28709/28709 [==============================] - 11s 380us/step</span><br><span class="line"></span><br><span class="line">Train Acc: 0.9182486253005799</span><br></pre></td></tr></table></figure></p>
<p>batch_size差不多是多少个数据更新一次参数，过大就没啥效果，过小梯度可能就会出现问题</p>
<p>于是我又调了一下参数，顺便加了一层Conv2D<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(Conv2D(<span class="number">25</span>, (<span class="number">6</span>, <span class="number">6</span>), input_shape=(x_train.shape[<span class="number">1</span>], x_train.shape[<span class="number">2</span>], x_train.shape[<span class="number">3</span>])))</span><br><span class="line">model.add(MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">model.add(Conv2D(<span class="number">25</span>, (<span class="number">6</span>, <span class="number">6</span>)))</span><br><span class="line">model.add(MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">model.add(Flatten())</span><br><span class="line">model.add(Dense(output_dim = <span class="number">100</span>))</span><br><span class="line">model.add(Activation(<span class="string">'relu'</span>))</span><br><span class="line">model.add(Dense(output_dim = <span class="number">7</span>))</span><br><span class="line">model.add(Activation(<span class="string">'softmax'</span>))</span><br><span class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>, optimizer=<span class="string">'adam'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">model.fit(x_train, y_train, batch_size = <span class="number">100</span>, epochs = <span class="number">20</span>)</span><br></pre></td></tr></table></figure></p>
<p>结果变好了一点<code>private:0.49651 public:0.50766</code><br>但是在Training Data上面的准确率居然可以到<code>0.94</code>？？？是不是Overfitted了</p>
<p>我增加了验证集，确实是Overfitted了<br>既然the deeper the better，那么我把他改的比较Deep了,然后可能参数过多了…train了40+min，结果正确率没变…跟蒙的正确率差不多…</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(x_train, y_train, x_val, y_val)</span>:</span></span><br><span class="line">    model = Sequential()</span><br><span class="line">    model.add(Conv2D(<span class="number">32</span>, (<span class="number">3</span>, <span class="number">3</span>), input_shape=(x_train.shape[<span class="number">1</span>], x_train.shape[<span class="number">2</span>], x_train.shape[<span class="number">3</span>])))</span><br><span class="line">    model.add(Conv2D(<span class="number">32</span>, (<span class="number">3</span>, <span class="number">3</span>)))</span><br><span class="line">    model.add(MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">    model.add(Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>)))</span><br><span class="line">    model.add(MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">    model.add(Conv2D(<span class="number">64</span>, (<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">    model.add(MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">    model.add(Conv2D(<span class="number">64</span>, (<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">    model.add(MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">    model.add(Flatten())</span><br><span class="line">    model.add(Dense(output_dim = <span class="number">100</span>))</span><br><span class="line">    <span class="comment">#model.add(Dropout(0.3))</span></span><br><span class="line">    model.add(Activation(<span class="string">'relu'</span>))</span><br><span class="line">    model.add(Dense(output_dim = <span class="number">7</span>))</span><br><span class="line">    model.add(Activation(<span class="string">'softmax'</span>))</span><br><span class="line">    model.compile(loss=<span class="string">'categorical_crossentropy'</span>, optimizer=<span class="string">'adam'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">    model.fit(x_train, y_train, batch_size = <span class="number">200</span>, epochs = <span class="number">10</span>)</span><br><span class="line">    result = model.evaluate(x_train, y_train, batch_size=<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">'\nTrain Acc:'</span>, result[<span class="number">1</span>])</span><br><span class="line">    result = model.evaluate(x_val, y_val, batch_size=<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">'\nVal Acc:'</span>, result[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<p>结果还是不理想<br><code>Train Acc: 0.4293785310734463 Val Acc: 0.38791915480018374</code>.<br><code>private 0.38088 public:0.40011</code>.</p>
<p>train一次真的好久…<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(x_train, y_train, x_val, y_val)</span>:</span></span><br><span class="line">    model = Sequential()</span><br><span class="line">    model.add(Conv2D(<span class="number">32</span>, (<span class="number">3</span>, <span class="number">3</span>), input_shape=(x_train.shape[<span class="number">1</span>], x_train.shape[<span class="number">2</span>], x_train.shape[<span class="number">3</span>])))</span><br><span class="line">    model.add(MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">    model.add(Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>)))</span><br><span class="line">    model.add(MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">    model.add(Conv2D(<span class="number">64</span>, (<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">    model.add(MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">    <span class="comment">#model.add(Conv2D(64, (2, 2)))</span></span><br><span class="line">    <span class="comment">#model.add(MaxPooling2D((2, 2)))</span></span><br><span class="line">    model.add(Flatten())</span><br><span class="line">    model.add(Dense(output_dim = <span class="number">100</span>))</span><br><span class="line">    model.add(Dropout(<span class="number">0.3</span>))</span><br><span class="line">    model.add(Activation(<span class="string">'relu'</span>))</span><br><span class="line">    model.add(Dense(output_dim = <span class="number">7</span>))</span><br><span class="line">    model.add(Activation(<span class="string">'softmax'</span>))</span><br><span class="line">    <span class="comment">#model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.1), metrics=['accuracy'])</span></span><br><span class="line">    model.compile(loss=<span class="string">'categorical_crossentropy'</span>, optimizer=<span class="string">'adam'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">    </span><br><span class="line">    model.fit(x_train, y_train, batch_size = <span class="number">200</span>, epochs = <span class="number">10</span>)</span><br><span class="line">    result = model.evaluate(x_train, y_train, batch_size=<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">'\nTrain Acc:'</span>, result[<span class="number">1</span>])</span><br><span class="line">    result = model.evaluate(x_val, y_val, batch_size=<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">'\nVal Acc:'</span>, result[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure></p>
<p><code>Train Acc: 0.5773211339433029Val Acc: 0.513435920992191</code>.<br><code>private: 0.49512 public:0.51518</code>.</p>
<p>把epochs改成30多Train一会<br><code>Train Acc: 0.9242537873106345 Val Acc: 0.5257234726688103</code>得分<code>private:0.51518 public:0.53998</code></p>
<p>我把之前的超复杂的model Train了100个epochs…结果没啥变化..噗…我可是train了5h 12-17 <code>private: 0.51908 public:0.53413</code><br><code>Train Acc: 0.9301534923253837 Val Acc: 0.5213596692696371</code>感觉又overfitted了？<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(x_train, y_train, x_val, y_val)</span>:</span></span><br><span class="line">    model = Sequential()</span><br><span class="line">    model.add(Conv2D(<span class="number">64</span>, (<span class="number">5</span>, <span class="number">5</span>), input_shape=(x_train.shape[<span class="number">1</span>], x_train.shape[<span class="number">2</span>], x_train.shape[<span class="number">3</span>])))</span><br><span class="line">    model.add(MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">    model.add(BatchNormalization(axis = <span class="number">-1</span>))</span><br><span class="line">    model.add(Dropout(<span class="number">0.3</span>))</span><br><span class="line"></span><br><span class="line">    model.add(Conv2D(<span class="number">128</span>, (<span class="number">5</span>, <span class="number">5</span>)))</span><br><span class="line">    model.add(MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">    model.add(BatchNormalization(axis = <span class="number">-1</span>))</span><br><span class="line">    model.add(Dropout(<span class="number">0.3</span>))</span><br><span class="line"></span><br><span class="line">    model.add(Conv2D(<span class="number">256</span>, (<span class="number">5</span>, <span class="number">5</span>)))</span><br><span class="line">    model.add(MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">    model.add(BatchNormalization(axis = <span class="number">-1</span>))</span><br><span class="line">    model.add(Dropout(<span class="number">0.3</span>))</span><br><span class="line">    <span class="comment">#model.add(Conv2D(64, (2, 2)))</span></span><br><span class="line">    <span class="comment">#model.add(MaxPooling2D((2, 2)))</span></span><br><span class="line">    model.add(Flatten())</span><br><span class="line">    model.add(Dense(output_dim = <span class="number">128</span>))</span><br><span class="line">    model.add(Dropout(<span class="number">0.5</span>))</span><br><span class="line">    model.add(Activation(<span class="string">'relu'</span>))</span><br><span class="line">    model.add(Dense(output_dim = <span class="number">7</span>))</span><br><span class="line">    model.add(Activation(<span class="string">'softmax'</span>))</span><br><span class="line">    <span class="comment">#model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.1), metrics=['accuracy'])</span></span><br><span class="line">    model.compile(loss=<span class="string">'categorical_crossentropy'</span>, optimizer=<span class="string">'adam'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure></p>
<p>吐了，train了一天，最好的结果<code>0.55642 public:0.56505</code>，改天来改改model再train8<br>—- Simple Baseline —-都没过orz</p>
<p>不管了，我一直train不起来，这是我最后train的一次model<br>主要是看<a href="https://github.com/JostineHo/mememoji" target="_blank" rel="noopener">这个</a>改了一下<br>怎么感觉是toolkit的问题…可能我姿势不太对？<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers.core <span class="keyword">import</span> Dense, Dropout, Activation</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Convolution2D, MaxPooling2D, Flatten, BatchNormalization</span><br><span class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> SGD, Adam</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> load_model</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">()</span>:</span></span><br><span class="line">    train_f = open(<span class="string">'train.csv'</span>, <span class="string">'r'</span>)</span><br><span class="line">    test_f = open(<span class="string">'test.csv'</span>, <span class="string">'r'</span>)</span><br><span class="line">    train_data = train_f.readlines()[<span class="number">1</span>:]</span><br><span class="line">    test_data = test_f.readlines()[<span class="number">1</span>:]</span><br><span class="line">    </span><br><span class="line">    x_train = []</span><br><span class="line">    y_train = []</span><br><span class="line">    x_test = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(train_data)):</span><br><span class="line">        train_data[i]=train_data[i].split(<span class="string">','</span>)</span><br><span class="line">        x_train.append(train_data[i][<span class="number">1</span>].split(<span class="string">' '</span>))</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(x_train[i])):</span><br><span class="line">            x_train[i][j]=int(x_train[i][j])</span><br><span class="line">        y_train.append(int(train_data[i][<span class="number">0</span>]))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(test_data)):</span><br><span class="line">        test_data[i]=test_data[i].split(<span class="string">','</span>)</span><br><span class="line">        x_test.append(test_data[i][<span class="number">1</span>].split(<span class="string">' '</span>))</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(x_test[i])):</span><br><span class="line">            x_test[i][j]=int(x_test[i][j])</span><br><span class="line">    </span><br><span class="line">    x = np.array(x_train)</span><br><span class="line">    y = np.array(y_train)</span><br><span class="line">    xx = np.array(x_test)</span><br><span class="line">    </span><br><span class="line">    x_train = x</span><br><span class="line">    y_train = y</span><br><span class="line">    x_test = xx</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># deal data</span></span><br><span class="line">    total_data = x_train.shape[<span class="number">0</span>]</span><br><span class="line">    x_train = x_train.reshape(total_data, <span class="number">48</span>, <span class="number">48</span>, <span class="number">1</span>)</span><br><span class="line">    x_test = x_test.reshape(x_test.shape[<span class="number">0</span>], <span class="number">48</span>, <span class="number">48</span>, <span class="number">1</span>)</span><br><span class="line">    x_train = x_train.astype(<span class="string">'float32'</span>)</span><br><span class="line">    x_test = x_test.astype(<span class="string">'float32'</span>)</span><br><span class="line">    x_train = x_train / <span class="number">255</span></span><br><span class="line">    x_test = x_test / <span class="number">255</span></span><br><span class="line">    </span><br><span class="line">    y_train = np_utils.to_categorical(y_train, <span class="number">7</span>)</span><br><span class="line">    </span><br><span class="line">    num = <span class="number">26000</span></span><br><span class="line">    x_val, y_val = x_train[num:, ], y_train[num:, ]</span><br><span class="line">    </span><br><span class="line">    x_train = x_train[:num, ]</span><br><span class="line">    y_train = y_train[:num, ]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> x_train, y_train, x_val, y_val, x_test</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(x_train, y_train, x_val, y_val)</span>:</span></span><br><span class="line">    model = Sequential()</span><br><span class="line">    model.add(Convolution2D(<span class="number">32</span>, <span class="number">3</span>, <span class="number">3</span>, input_shape=(x_train.shape[<span class="number">1</span>], x_train.shape[<span class="number">2</span>], x_train.shape[<span class="number">3</span>])))</span><br><span class="line">    model.add(Convolution2D(<span class="number">32</span>, <span class="number">3</span>, <span class="number">3</span>, border_mode=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">    model.add(Convolution2D(<span class="number">32</span>, <span class="number">3</span>, <span class="number">3</span>, border_mode=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">    model.add(MaxPooling2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">    </span><br><span class="line">    model.add(Convolution2D(<span class="number">64</span>, <span class="number">3</span>, <span class="number">3</span>, border_mode=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">    model.add(Convolution2D(<span class="number">64</span>, <span class="number">3</span>, <span class="number">3</span>, border_mode=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">    model.add(Convolution2D(<span class="number">64</span>, <span class="number">3</span>, <span class="number">3</span>, border_mode=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">    model.add(MaxPooling2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">    </span><br><span class="line">    model.add(Convolution2D(<span class="number">128</span>, <span class="number">3</span>, <span class="number">3</span>, border_mode=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">    model.add(Convolution2D(<span class="number">128</span>, <span class="number">3</span>, <span class="number">3</span>, border_mode=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">    model.add(Convolution2D(<span class="number">128</span>, <span class="number">3</span>, <span class="number">3</span>, border_mode=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">    model.add(MaxPooling2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">    </span><br><span class="line">    model.add(Flatten())</span><br><span class="line">    model.add(Dense(units = <span class="number">64</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">    model.add(Dropout(<span class="number">0.5</span>))</span><br><span class="line">    model.add(Dense(units = <span class="number">64</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">    model.add(Dropout(<span class="number">0.5</span>))</span><br><span class="line">        </span><br><span class="line">    model.add(Dense(units = <span class="number">7</span>, activation=<span class="string">'softmax'</span>))</span><br><span class="line">    <span class="comment">#model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.1), metrics=['accuracy'])</span></span><br><span class="line">    model.compile(loss=<span class="string">'categorical_crossentropy'</span>, optimizer=<span class="string">'adam'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">    </span><br><span class="line">    model.fit(x_train, y_train, nb_epoch=<span class="number">50</span>, batch_size=<span class="number">128</span>,</span><br><span class="line">          validation_split=<span class="number">0.3</span>, shuffle=<span class="keyword">True</span>, verbose=<span class="number">1</span>)</span><br><span class="line">    model.save(<span class="string">'my_model.h5'</span>)</span><br><span class="line">    loss_and_metrics = model.evaluate(x_train, y_train, batch_size=<span class="number">128</span>, verbose=<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">'Done!'</span>)</span><br><span class="line">    print(<span class="string">'Loss: '</span>, loss_and_metrics[<span class="number">0</span>])</span><br><span class="line">    print(<span class="string">' Acc: '</span>, loss_and_metrics[<span class="number">1</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(model, x_test)</span>:</span></span><br><span class="line">    y_test = model.predict(x_test)</span><br><span class="line">    test_f = open(<span class="string">'y_test.csv'</span>, <span class="string">'w'</span>)</span><br><span class="line">    print(<span class="string">'id,label'</span>, file=test_f)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(y_test)):</span><br><span class="line">        print(i, np.argmax(y_test[i]), sep=<span class="string">','</span>, file=test_f)</span><br><span class="line">    test_f.close()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    x_train, y_train, x_val, y_val, x_test = load_data()</span><br><span class="line">    print(<span class="string">'load_data() ok'</span>)</span><br><span class="line">    model = train(x_train, y_train, x_val, y_val)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">'train() ok'</span>)</span><br><span class="line">    test(model, x_test)</span><br><span class="line">    print(<span class="string">'test() ok'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure></p>
<h3 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h3><p>最后来做一下，有趣的数据和Model的可视化</p>
<p>training data前十张图片</p>
<style>
.img-wrap{
    border: 1px 
}
img{
    float: left;
    width: 20%;
}
</style>

<div class="img-wrap">
<img src="/img/ml-practice/hw3/pic0.jpg">
<img src="/img/ml-practice/hw3/pic1.jpg">
<img src="/img/ml-practice/hw3/pic2.jpg">
<img src="/img/ml-practice/hw3/pic3.jpg">
<img src="/img/ml-practice/hw3/pic4.jpg">
</div>

<div class="img-wrap">
<img src="/img/ml-practice/hw3/pic5.jpg">
<img src="/img/ml-practice/hw3/pic6.jpg">
<img src="/img/ml-practice/hw3/pic7.jpg">
<img src="/img/ml-practice/hw3/pic8.jpg">
<img src="/img/ml-practice/hw3/pic9.jpg">
</div>

<p>标签分别是<br>0 Angry 1 Angry 2 Fear 3 Sad 4 Neutral<br>5 Fear 6 Sad 7 Happy 8 Happy 9 Fear</p>
<p>test data前十张照片</p>
<div class="img-wrap">
<img src="/img/ml-practice/hw3/test_pic0.jpg">
<img src="/img/ml-practice/hw3/test_pic1.jpg">
<img src="/img/ml-practice/hw3/test_pic2.jpg">
<img src="/img/ml-practice/hw3/test_pic3.jpg">
<img src="/img/ml-practice/hw3/test_pic4.jpg">
</div>
<div class="img-wrap">
<img src="/img/ml-practice/hw3/test_pic5.jpg">
<img src="/img/ml-practice/hw3/test_pic6.jpg">
<img src="/img/ml-practice/hw3/test_pic7.jpg">
<img src="/img/ml-practice/hw3/test_pic8.jpg">
<img src="/img/ml-practice/hw3/test_pic9.jpg">
</div>


<p>我给出了y_test<br>0 Happy 1 Happy 2 Happy 3 Happy 4 Neutral<br>5 Surprise 6 Angry 7 Neutral 8 Fear 9 Neutral</p>
<p>感觉还挺对的啊233<br>相关代码<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show</span><span class="params">(x_train, y_train, x_val, y_val, x_test)</span>:</span></span><br><span class="line">    emo = [<span class="string">'Angry'</span>, <span class="string">'Disgust'</span>, <span class="string">'Fear'</span>, <span class="string">'Happy'</span>,</span><br><span class="line">           <span class="string">'Sad'</span>, <span class="string">'Surprise'</span>, <span class="string">'Neutral'</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        plt.imshow(x_train[i])</span><br><span class="line">        plt.axis(<span class="string">'off'</span>)</span><br><span class="line">        plt.savefig(<span class="string">'pic'</span>+str(i)+<span class="string">'.jpg'</span>)</span><br><span class="line">        plt.show()</span><br><span class="line">        print(i, emo[y_train[i]])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        plt.imshow(x_test[i])</span><br><span class="line">        plt.axis(<span class="string">'off'</span>)</span><br><span class="line">        plt.savefig(<span class="string">'test_pic'</span>+str(i)+<span class="string">'.jpg'</span>)</span><br><span class="line">        plt.show()</span><br></pre></td></tr></table></figure></p>
<p>下面就是有关Model的部分</p>
<p>这是train出最好结果的Model结构<code>print(model.summary())</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">conv2d_1 (Conv2D)            (None, 44, 44, 32)        832       </span><br><span class="line">_________________________________________________________________</span><br><span class="line">max_pooling2d_1 (MaxPooling2 (None, 22, 22, 32)        0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">batch_normalization_1 (Batch (None, 22, 22, 32)        128       </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dropout_1 (Dropout)          (None, 22, 22, 32)        0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">conv2d_2 (Conv2D)            (None, 18, 18, 64)        51264     </span><br><span class="line">_________________________________________________________________</span><br><span class="line">max_pooling2d_2 (MaxPooling2 (None, 9, 9, 64)          0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">batch_normalization_2 (Batch (None, 9, 9, 64)          256       </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dropout_2 (Dropout)          (None, 9, 9, 64)          0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">conv2d_3 (Conv2D)            (None, 7, 7, 128)         73856     </span><br><span class="line">_________________________________________________________________</span><br><span class="line">max_pooling2d_3 (MaxPooling2 (None, 3, 3, 128)         0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">batch_normalization_3 (Batch (None, 3, 3, 128)         512       </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dropout_3 (Dropout)          (None, 3, 3, 128)         0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">conv2d_4 (Conv2D)            (None, 2, 2, 128)         65664     </span><br><span class="line">_________________________________________________________________</span><br><span class="line">max_pooling2d_4 (MaxPooling2 (None, 1, 1, 128)         0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">flatten_1 (Flatten)          (None, 128)               0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_1 (Dense)              (None, 128)               16512     </span><br><span class="line">_________________________________________________________________</span><br><span class="line">activation_1 (Activation)    (None, 128)               0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dropout_4 (Dropout)          (None, 128)               0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_2 (Dense)              (None, 64)                8256      </span><br><span class="line">_________________________________________________________________</span><br><span class="line">activation_2 (Activation)    (None, 64)                0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dropout_5 (Dropout)          (None, 64)                0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_3 (Dense)              (None, 7)                 455       </span><br><span class="line">_________________________________________________________________</span><br><span class="line">activation_3 (Activation)    (None, 7)                 0         </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 217,735</span><br><span class="line">Trainable params: 217,287</span><br><span class="line">Non-trainable params: 448</span><br><span class="line">_________________________________________________________________</span><br><span class="line">None</span><br></pre></td></tr></table></figure>
<p>第一个卷积层参数<code>print(model.get_layer(&#39;conv2d_1&#39;).get_weights())</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br></pre></td><td class="code"><pre><span class="line">[array([[[[ <span class="number">1.98697541e-02</span>,  <span class="number">7.08070025e-02</span>,  <span class="number">2.13078111e-02</span>,</span><br><span class="line">          <span class="number">-5.15683815e-02</span>,  <span class="number">1.00081712e-01</span>,  <span class="number">2.15147361e-02</span>,</span><br><span class="line">           <span class="number">5.15370332e-02</span>, <span class="number">-5.25256731e-02</span>, <span class="number">-7.89779201e-02</span>,</span><br><span class="line">           <span class="number">6.46693213e-03</span>,  <span class="number">3.96410637e-02</span>, <span class="number">-5.25688417e-02</span>,</span><br><span class="line">           <span class="number">1.44804746e-01</span>, <span class="number">-7.12847039e-02</span>,  <span class="number">1.06713027e-01</span>,</span><br><span class="line">           <span class="number">1.60096228e-01</span>,  <span class="number">1.89897269e-01</span>, <span class="number">-8.76477435e-02</span>,</span><br><span class="line">          <span class="number">-2.29081400e-02</span>, <span class="number">-3.11734769e-02</span>, <span class="number">-1.93705603e-01</span>,</span><br><span class="line">          <span class="number">-6.44565597e-02</span>, <span class="number">-5.65660931e-03</span>, <span class="number">-1.87695608e-01</span>,</span><br><span class="line">           <span class="number">1.21000633e-01</span>, <span class="number">-6.11960106e-02</span>, <span class="number">-4.44339998e-02</span>,</span><br><span class="line">           <span class="number">1.05157614e-01</span>, <span class="number">-1.10467389e-01</span>, <span class="number">-2.06037536e-01</span>,</span><br><span class="line">           <span class="number">2.08152682e-01</span>,  <span class="number">1.53192570e-02</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">-7.94749632e-02</span>, <span class="number">-1.10797726e-01</span>, <span class="number">-2.95411889e-02</span>,</span><br><span class="line">           <span class="number">1.05928695e-02</span>, <span class="number">-9.10679698e-02</span>, <span class="number">-1.85829073e-01</span>,</span><br><span class="line">           <span class="number">2.02179357e-01</span>,  <span class="number">1.05242565e-01</span>, <span class="number">-1.98338836e-01</span>,</span><br><span class="line">           <span class="number">6.08536191e-02</span>, <span class="number">-3.61365639e-02</span>,  <span class="number">5.57313822e-02</span>,</span><br><span class="line">          <span class="number">-1.43119842e-02</span>,  <span class="number">3.56776677e-02</span>, <span class="number">-6.43971115e-02</span>,</span><br><span class="line">           <span class="number">1.39809683e-01</span>,  <span class="number">8.17038235e-04</span>, <span class="number">-1.02193907e-01</span>,</span><br><span class="line">          <span class="number">-8.24483410e-02</span>, <span class="number">-1.46627268e-02</span>, <span class="number">-3.01948309e-01</span>,</span><br><span class="line">          <span class="number">-5.91862574e-02</span>,  <span class="number">7.99618885e-02</span>,  <span class="number">2.42080484e-02</span>,</span><br><span class="line">           <span class="number">9.46005136e-02</span>,  <span class="number">2.90510654e-01</span>,  <span class="number">4.47756145e-04</span>,</span><br><span class="line">          <span class="number">-1.72438115e-01</span>, <span class="number">-1.18252495e-03</span>, <span class="number">-5.75673319e-02</span>,</span><br><span class="line">           <span class="number">1.69455364e-01</span>,  <span class="number">3.26001421e-02</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">-4.84515466e-02</span>, <span class="number">-9.15088356e-02</span>, <span class="number">-5.86454906e-02</span>,</span><br><span class="line">          <span class="number">-1.31094322e-01</span>, <span class="number">-6.93618506e-03</span>, <span class="number">-2.83256590e-01</span>,</span><br><span class="line">          <span class="number">-1.69737684e-03</span>, <span class="number">-1.20050490e-01</span>,  <span class="number">2.41740972e-01</span>,</span><br><span class="line">           <span class="number">9.86552760e-02</span>, <span class="number">-9.77197811e-02</span>,  <span class="number">1.34425595e-01</span>,</span><br><span class="line">          <span class="number">-9.40313116e-02</span>,  <span class="number">1.00749932e-01</span>,  <span class="number">1.31006762e-02</span>,</span><br><span class="line">           <span class="number">9.30177867e-02</span>, <span class="number">-1.66146964e-01</span>,  <span class="number">5.57237025e-03</span>,</span><br><span class="line">           <span class="number">7.31411353e-02</span>, <span class="number">-8.76804162e-03</span>,  <span class="number">1.37711242e-01</span>,</span><br><span class="line">          <span class="number">-4.58096415e-02</span>,  <span class="number">2.22220905e-02</span>,  <span class="number">1.01391926e-01</span>,</span><br><span class="line">          <span class="number">-7.97675103e-02</span>,  <span class="number">3.93110923e-02</span>,  <span class="number">8.29865634e-02</span>,</span><br><span class="line">          <span class="number">-1.85488507e-01</span>,  <span class="number">1.11202924e-02</span>,  <span class="number">3.66388783e-02</span>,</span><br><span class="line">           <span class="number">1.33667052e-01</span>,  <span class="number">1.18211597e-01</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">-4.10749484e-03</span>,  <span class="number">1.25970155e-01</span>,  <span class="number">3.67107615e-02</span>,</span><br><span class="line">           <span class="number">4.58605774e-02</span>, <span class="number">-8.73558037e-03</span>, <span class="number">-1.11491427e-01</span>,</span><br><span class="line">          <span class="number">-2.09319130e-01</span>, <span class="number">-3.44122830e-03</span>,  <span class="number">5.52160554e-02</span>,</span><br><span class="line">           <span class="number">1.17073983e-01</span>,  <span class="number">1.50324449e-01</span>,  <span class="number">1.80855498e-01</span>,</span><br><span class="line">          <span class="number">-1.18359365e-01</span>,  <span class="number">1.13875926e-01</span>, <span class="number">-7.70208091e-02</span>,</span><br><span class="line">           <span class="number">5.23166358e-02</span>, <span class="number">-2.35963296e-02</span>,  <span class="number">4.03642021e-02</span>,</span><br><span class="line">           <span class="number">1.39719369e-02</span>,  <span class="number">2.40484951e-03</span>,  <span class="number">1.66882589e-01</span>,</span><br><span class="line">           <span class="number">2.26039141e-02</span>, <span class="number">-2.48743184e-02</span>, <span class="number">-1.94001179e-02</span>,</span><br><span class="line">           <span class="number">2.79328898e-02</span>,  <span class="number">6.15491420e-02</span>, <span class="number">-2.51877345e-02</span>,</span><br><span class="line">           <span class="number">6.85293749e-02</span>, <span class="number">-5.04620709e-02</span>,  <span class="number">2.18394846e-02</span>,</span><br><span class="line">          <span class="number">-7.98307136e-02</span>,  <span class="number">1.57275852e-02</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">-4.24592867e-02</span>, <span class="number">-5.98774999e-02</span>,  <span class="number">6.20517395e-02</span>,</span><br><span class="line">           <span class="number">6.69216141e-02</span>,  <span class="number">1.39782215e-02</span>,  <span class="number">1.39022190e-02</span>,</span><br><span class="line">          <span class="number">-8.40072632e-02</span>, <span class="number">-9.28316936e-02</span>, <span class="number">-3.81821878e-02</span>,</span><br><span class="line">           <span class="number">3.28571610e-02</span>, <span class="number">-9.50155482e-02</span>,  <span class="number">2.06548795e-01</span>,</span><br><span class="line">          <span class="number">-5.07548172e-03</span>, <span class="number">-8.27670172e-02</span>,  <span class="number">1.74515531e-03</span>,</span><br><span class="line">           <span class="number">7.49436691e-02</span>,  <span class="number">5.22173010e-02</span>, <span class="number">-1.54335704e-02</span>,</span><br><span class="line">           <span class="number">6.05179043e-03</span>, <span class="number">-2.36486383e-02</span>,  <span class="number">4.83768359e-02</span>,</span><br><span class="line">          <span class="number">-1.98077876e-02</span>, <span class="number">-5.95450103e-02</span>, <span class="number">-3.22199836e-02</span>,</span><br><span class="line">           <span class="number">1.26348529e-02</span>, <span class="number">-9.37070101e-02</span>, <span class="number">-1.34252422e-02</span>,</span><br><span class="line">           <span class="number">1.63472250e-01</span>,  <span class="number">2.09844895e-02</span>,  <span class="number">4.21280973e-02</span>,</span><br><span class="line">          <span class="number">-1.49431959e-01</span>,  <span class="number">9.01276767e-02</span>]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">       [[[ <span class="number">3.45688313e-02</span>,  <span class="number">7.97107220e-02</span>,  <span class="number">3.33844125e-02</span>,</span><br><span class="line">          <span class="number">-1.20801199e-02</span>,  <span class="number">2.63867676e-02</span>,  <span class="number">2.75757816e-02</span>,</span><br><span class="line">          <span class="number">-7.70123675e-02</span>, <span class="number">-3.14535312e-02</span>, <span class="number">-2.06373960e-01</span>,</span><br><span class="line">           <span class="number">1.05471522e-01</span>,  <span class="number">4.95559983e-02</span>, <span class="number">-1.16412774e-01</span>,</span><br><span class="line">          <span class="number">-6.32849857e-02</span>,  <span class="number">2.04185806e-02</span>, <span class="number">-9.10404772e-02</span>,</span><br><span class="line">           <span class="number">8.10925439e-02</span>,  <span class="number">1.11957282e-01</span>, <span class="number">-1.06745861e-01</span>,</span><br><span class="line">          <span class="number">-5.45982271e-02</span>, <span class="number">-2.82833027e-03</span>, <span class="number">-2.66196430e-01</span>,</span><br><span class="line">           <span class="number">1.64179057e-01</span>,  <span class="number">1.23541646e-01</span>, <span class="number">-1.65091053e-01</span>,</span><br><span class="line">           <span class="number">7.54139572e-02</span>,  <span class="number">2.50726908e-01</span>, <span class="number">-5.71109504e-02</span>,</span><br><span class="line">           <span class="number">1.48351267e-01</span>, <span class="number">-3.64737064e-02</span>,  <span class="number">1.51068226e-01</span>,</span><br><span class="line">          <span class="number">-4.74842303e-02</span>,  <span class="number">1.05010234e-01</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">4.32509072e-02</span>, <span class="number">-1.08206309e-01</span>, <span class="number">-5.32749258e-02</span>,</span><br><span class="line">           <span class="number">8.09469670e-02</span>, <span class="number">-1.45653322e-01</span>,  <span class="number">1.83570251e-01</span>,</span><br><span class="line">           <span class="number">4.24414650e-02</span>,  <span class="number">1.59827664e-01</span>,  <span class="number">2.48608693e-01</span>,</span><br><span class="line">           <span class="number">7.29027241e-02</span>,  <span class="number">3.15706767e-02</span>, <span class="number">-1.96331352e-01</span>,</span><br><span class="line">          <span class="number">-1.85681745e-01</span>, <span class="number">-1.96331993e-01</span>,  <span class="number">8.14330503e-02</span>,</span><br><span class="line">          <span class="number">-5.10441847e-02</span>,  <span class="number">1.26650885e-01</span>,  <span class="number">3.09712440e-01</span>,</span><br><span class="line">          <span class="number">-4.17465568e-02</span>, <span class="number">-5.22688702e-02</span>,  <span class="number">1.27634957e-01</span>,</span><br><span class="line">           <span class="number">1.45883352e-01</span>,  <span class="number">8.01252499e-02</span>,  <span class="number">3.01035553e-01</span>,</span><br><span class="line">          <span class="number">-4.15359400e-02</span>,  <span class="number">3.02916348e-01</span>, <span class="number">-2.24125832e-02</span>,</span><br><span class="line">          <span class="number">-5.01342490e-02</span>,  <span class="number">1.41805753e-01</span>,  <span class="number">2.88155973e-01</span>,</span><br><span class="line">          <span class="number">-6.53511472e-03</span>,  <span class="number">8.58637840e-02</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">-1.38286784e-01</span>, <span class="number">-4.97966297e-02</span>, <span class="number">-1.29996330e-01</span>,</span><br><span class="line">           <span class="number">4.16094959e-02</span>,  <span class="number">1.82389528e-01</span>, <span class="number">-2.68514678e-02</span>,</span><br><span class="line">           <span class="number">2.51998603e-01</span>, <span class="number">-1.14475004e-01</span>,  <span class="number">2.60868251e-01</span>,</span><br><span class="line">          <span class="number">-3.40055041e-02</span>, <span class="number">-1.70233380e-02</span>, <span class="number">-1.62914380e-01</span>,</span><br><span class="line">          <span class="number">-2.20064163e-01</span>, <span class="number">-2.13668033e-01</span>,  <span class="number">1.09006934e-01</span>,</span><br><span class="line">          <span class="number">-1.12826496e-01</span>, <span class="number">-2.98617661e-01</span>,  <span class="number">2.04329818e-01</span>,</span><br><span class="line">           <span class="number">1.35896914e-02</span>,  <span class="number">6.32194150e-03</span>,  <span class="number">1.94850698e-01</span>,</span><br><span class="line">           <span class="number">5.52918576e-02</span>, <span class="number">-5.35391085e-02</span>,  <span class="number">7.24675506e-02</span>,</span><br><span class="line">          <span class="number">-3.01813986e-02</span>,  <span class="number">1.07929237e-01</span>,  <span class="number">1.62357897e-01</span>,</span><br><span class="line">          <span class="number">-2.60545492e-01</span>, <span class="number">-2.00226828e-02</span>,  <span class="number">2.04357430e-01</span>,</span><br><span class="line">          <span class="number">-9.20708328e-02</span>,  <span class="number">2.05012739e-01</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">-5.31569403e-03</span>,  <span class="number">1.90459922e-01</span>, <span class="number">-1.82926297e-01</span>,</span><br><span class="line">          <span class="number">-1.36217758e-01</span>,  <span class="number">6.76145703e-02</span>, <span class="number">-2.20544443e-01</span>,</span><br><span class="line">          <span class="number">-2.50425451e-02</span>,  <span class="number">1.04731329e-01</span>, <span class="number">-2.30462104e-01</span>,</span><br><span class="line">          <span class="number">-2.48308837e-01</span>,  <span class="number">1.51725367e-01</span>, <span class="number">-2.57288784e-01</span>,</span><br><span class="line">          <span class="number">-8.46081376e-02</span>,  <span class="number">1.50209665e-01</span>,  <span class="number">6.37715831e-02</span>,</span><br><span class="line">           <span class="number">3.92922014e-02</span>,  <span class="number">9.87009183e-02</span>,  <span class="number">1.67022254e-02</span>,</span><br><span class="line">           <span class="number">6.77632987e-02</span>, <span class="number">-1.95640363e-02</span>, <span class="number">-2.88411397e-02</span>,</span><br><span class="line">          <span class="number">-1.35260761e-01</span>, <span class="number">-8.90125260e-02</span>, <span class="number">-1.68812513e-01</span>,</span><br><span class="line">           <span class="number">5.60835637e-02</span>, <span class="number">-1.15505241e-01</span>, <span class="number">-4.61194385e-03</span>,</span><br><span class="line">           <span class="number">1.25047281e-01</span>, <span class="number">-1.84047252e-01</span>,  <span class="number">9.05039459e-02</span>,</span><br><span class="line">           <span class="number">1.28791839e-01</span>,  <span class="number">1.42685696e-01</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">1.28499687e-01</span>, <span class="number">-8.88020843e-02</span>, <span class="number">-7.54232705e-02</span>,</span><br><span class="line">          <span class="number">-2.42703990e-03</span>, <span class="number">-1.13100246e-01</span>, <span class="number">-1.24508232e-01</span>,</span><br><span class="line">          <span class="number">-9.55692455e-02</span>,  <span class="number">1.04941204e-01</span>, <span class="number">-1.64641496e-02</span>,</span><br><span class="line">          <span class="number">-2.12375224e-01</span>, <span class="number">-1.05855174e-01</span>, <span class="number">-6.46060929e-02</span>,</span><br><span class="line">           <span class="number">1.16432726e-01</span>,  <span class="number">3.79320197e-02</span>, <span class="number">-7.94462860e-02</span>,</span><br><span class="line">           <span class="number">6.03967756e-02</span>,  <span class="number">2.62370169e-01</span>,  <span class="number">1.01605114e-02</span>,</span><br><span class="line">          <span class="number">-5.71687818e-02</span>,  <span class="number">2.33785566e-02</span>,  <span class="number">1.01750411e-01</span>,</span><br><span class="line">          <span class="number">-9.63896811e-02</span>, <span class="number">-7.12030604e-02</span>,  <span class="number">3.58088724e-02</span>,</span><br><span class="line">          <span class="number">-8.03487822e-02</span>, <span class="number">-8.63136053e-02</span>, <span class="number">-4.95648608e-02</span>,</span><br><span class="line">           <span class="number">7.65802786e-02</span>, <span class="number">-1.39876679e-01</span>, <span class="number">-3.15147713e-02</span>,</span><br><span class="line">          <span class="number">-7.21401721e-03</span>,  <span class="number">1.37567632e-02</span>]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">       [[[ <span class="number">8.77455696e-02</span>,  <span class="number">5.97124249e-02</span>, <span class="number">-1.89694241e-01</span>,</span><br><span class="line">           <span class="number">3.23649533e-02</span>, <span class="number">-1.92350626e-01</span>,  <span class="number">1.24370664e-01</span>,</span><br><span class="line">          <span class="number">-1.71399102e-01</span>, <span class="number">-1.43710732e-01</span>, <span class="number">-5.04495315e-02</span>,</span><br><span class="line">          <span class="number">-1.09340347e-01</span>,  <span class="number">2.09975004e-01</span>,  <span class="number">1.20991776e-02</span>,</span><br><span class="line">           <span class="number">4.99377362e-02</span>,  <span class="number">4.60348763e-02</span>, <span class="number">-2.89339144e-02</span>,</span><br><span class="line">           <span class="number">4.50112969e-02</span>, <span class="number">-4.30244394e-03</span>,  <span class="number">1.93148911e-01</span>,</span><br><span class="line">           <span class="number">3.53351355e-01</span>,  <span class="number">4.69863154e-02</span>, <span class="number">-9.06451568e-02</span>,</span><br><span class="line">          <span class="number">-9.48860496e-02</span>,  <span class="number">2.56411016e-01</span>,  <span class="number">1.34009928e-01</span>,</span><br><span class="line">          <span class="number">-1.25456363e-01</span>,  <span class="number">7.16938078e-02</span>, <span class="number">-9.46179405e-02</span>,</span><br><span class="line">           <span class="number">1.81610599e-01</span>,  <span class="number">8.04117322e-02</span>,  <span class="number">2.65546918e-01</span>,</span><br><span class="line">           <span class="number">1.72054153e-02</span>, <span class="number">-5.89300804e-02</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">1.96502239e-01</span>, <span class="number">-1.39927343e-01</span>, <span class="number">-1.33911312e-01</span>,</span><br><span class="line">           <span class="number">1.26991317e-01</span>,  <span class="number">1.59659669e-01</span>,  <span class="number">2.67307848e-01</span>,</span><br><span class="line">          <span class="number">-6.45235255e-02</span>,  <span class="number">3.04940462e-01</span>,  <span class="number">2.59633511e-01</span>,</span><br><span class="line">          <span class="number">-1.34681433e-01</span>, <span class="number">-6.99868128e-02</span>,  <span class="number">1.27351627e-01</span>,</span><br><span class="line">           <span class="number">2.96805538e-02</span>,  <span class="number">2.23741010e-01</span>, <span class="number">-2.36314595e-01</span>,</span><br><span class="line">           <span class="number">1.37561960e-02</span>,  <span class="number">1.17661051e-01</span>,  <span class="number">7.28028081e-03</span>,</span><br><span class="line">           <span class="number">3.55358928e-01</span>,  <span class="number">1.30962580e-01</span>,  <span class="number">2.41935819e-01</span>,</span><br><span class="line">           <span class="number">6.30789669e-03</span>, <span class="number">-3.37278545e-02</span>,  <span class="number">2.55262971e-01</span>,</span><br><span class="line">          <span class="number">-3.57140094e-01</span>,  <span class="number">1.59507662e-01</span>, <span class="number">-1.46307647e-01</span>,</span><br><span class="line">           <span class="number">7.23242164e-02</span>,  <span class="number">1.81715831e-01</span>,  <span class="number">1.53966248e-01</span>,</span><br><span class="line">           <span class="number">5.50780967e-02</span>,  <span class="number">2.59742830e-02</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">-4.03862726e-03</span>, <span class="number">-8.40769988e-03</span>,  <span class="number">4.09948453e-02</span>,</span><br><span class="line">          <span class="number">-1.06060386e-01</span>,  <span class="number">2.83326656e-01</span>,  <span class="number">3.28720927e-01</span>,</span><br><span class="line">           <span class="number">2.72140056e-01</span>, <span class="number">-2.37294659e-01</span>, <span class="number">-7.40716830e-02</span>,</span><br><span class="line">          <span class="number">-2.88817793e-01</span>, <span class="number">-6.50088638e-02</span>, <span class="number">-4.09717560e-02</span>,</span><br><span class="line">           <span class="number">2.44265974e-01</span>, <span class="number">-2.33402833e-01</span>, <span class="number">-1.10050410e-01</span>,</span><br><span class="line">           <span class="number">3.10584600e-03</span>, <span class="number">-2.57611006e-01</span>, <span class="number">-4.29998398e-01</span>,</span><br><span class="line">           <span class="number">2.15267524e-01</span>,  <span class="number">1.15541726e-01</span>, <span class="number">-8.29731598e-02</span>,</span><br><span class="line">           <span class="number">1.27682254e-01</span>, <span class="number">-2.33266324e-01</span>, <span class="number">-1.31910518e-01</span>,</span><br><span class="line">          <span class="number">-2.38840252e-01</span>, <span class="number">-1.11221030e-01</span>,  <span class="number">2.74591565e-01</span>,</span><br><span class="line">          <span class="number">-1.67183250e-01</span>,  <span class="number">1.53959781e-01</span>, <span class="number">-2.62739919e-02</span>,</span><br><span class="line">          <span class="number">-5.13264649e-02</span>,  <span class="number">9.53476317e-03</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">-2.91227967e-01</span>,  <span class="number">1.92851841e-01</span>,  <span class="number">6.34998009e-02</span>,</span><br><span class="line">          <span class="number">-4.25702594e-02</span>, <span class="number">-1.84211820e-01</span>,  <span class="number">2.67261922e-01</span>,</span><br><span class="line">           <span class="number">1.99890241e-01</span>, <span class="number">-1.15273908e-01</span>, <span class="number">-2.64678597e-01</span>,</span><br><span class="line">          <span class="number">-1.63764969e-01</span>,  <span class="number">3.91174294e-02</span>, <span class="number">-3.50707620e-02</span>,</span><br><span class="line">           <span class="number">2.68421412e-01</span>, <span class="number">-1.73132405e-01</span>,  <span class="number">2.80285906e-02</span>,</span><br><span class="line">          <span class="number">-2.66031735e-02</span>, <span class="number">-1.72160462e-01</span>, <span class="number">-2.76752502e-01</span>,</span><br><span class="line">           <span class="number">1.25095621e-01</span>,  <span class="number">1.37536764e-01</span>, <span class="number">-1.05138674e-01</span>,</span><br><span class="line">           <span class="number">1.68358728e-01</span>, <span class="number">-1.84328616e-01</span>, <span class="number">-2.26817891e-01</span>,</span><br><span class="line">          <span class="number">-2.03812003e-01</span>, <span class="number">-8.87360126e-02</span>,  <span class="number">4.47154827e-02</span>,</span><br><span class="line">          <span class="number">-5.68596832e-02</span>,  <span class="number">3.41318071e-01</span>, <span class="number">-1.64236873e-01</span>,</span><br><span class="line">           <span class="number">2.29175854e-02</span>,  <span class="number">1.24349169e-01</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">-1.50303990e-01</span>, <span class="number">-1.24774635e-01</span>, <span class="number">-8.22538063e-02</span>,</span><br><span class="line">           <span class="number">5.53404493e-03</span>, <span class="number">-1.11148380e-01</span>, <span class="number">-2.94404346e-02</span>,</span><br><span class="line">          <span class="number">-1.43259659e-03</span>,  <span class="number">1.85924873e-01</span>,  <span class="number">1.08227178e-01</span>,</span><br><span class="line">           <span class="number">1.17234342e-01</span>, <span class="number">-4.91014384e-02</span>, <span class="number">-2.44117349e-01</span>,</span><br><span class="line">           <span class="number">9.34721082e-02</span>,  <span class="number">2.39500254e-01</span>,  <span class="number">2.65940160e-01</span>,</span><br><span class="line">          <span class="number">-1.61937177e-02</span>,  <span class="number">1.31614238e-01</span>, <span class="number">-7.58006722e-02</span>,</span><br><span class="line">           <span class="number">1.05317712e-01</span>, <span class="number">-4.97416444e-02</span>,  <span class="number">1.06382117e-01</span>,</span><br><span class="line">           <span class="number">1.55792519e-01</span>, <span class="number">-6.84151053e-02</span>,  <span class="number">1.45529345e-01</span>,</span><br><span class="line">           <span class="number">2.03677062e-02</span>, <span class="number">-8.97034109e-02</span>, <span class="number">-2.06795484e-01</span>,</span><br><span class="line">           <span class="number">1.54043451e-01</span>,  <span class="number">2.10126981e-01</span>, <span class="number">-8.91636238e-02</span>,</span><br><span class="line">           <span class="number">1.28453285e-01</span>,  <span class="number">9.36990604e-02</span>]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">       [[[<span class="number">-1.62867770e-01</span>,  <span class="number">1.78554595e-01</span>,  <span class="number">1.07262485e-01</span>,</span><br><span class="line">          <span class="number">-2.01251507e-01</span>, <span class="number">-1.86686456e-01</span>, <span class="number">-1.39267615e-03</span>,</span><br><span class="line">          <span class="number">-2.91764736e-01</span>, <span class="number">-6.71359822e-02</span>,  <span class="number">9.27731097e-02</span>,</span><br><span class="line">          <span class="number">-4.40010466e-02</span>,  <span class="number">1.04116030e-01</span>,  <span class="number">9.00273323e-02</span>,</span><br><span class="line">          <span class="number">-9.75889117e-02</span>, <span class="number">-9.58028287e-02</span>,  <span class="number">1.44259349e-01</span>,</span><br><span class="line">           <span class="number">1.72845557e-01</span>, <span class="number">-7.35422820e-02</span>, <span class="number">-5.54325320e-02</span>,</span><br><span class="line">          <span class="number">-5.81960455e-02</span>, <span class="number">-9.40498710e-03</span>,  <span class="number">2.49349073e-01</span>,</span><br><span class="line">           <span class="number">2.98932809e-02</span>,  <span class="number">1.47391170e-01</span>,  <span class="number">2.23646581e-01</span>,</span><br><span class="line">          <span class="number">-1.66808844e-01</span>, <span class="number">-2.98017204e-01</span>,  <span class="number">1.76479578e-01</span>,</span><br><span class="line">           <span class="number">9.54928771e-02</span>, <span class="number">-1.94601282e-01</span>,  <span class="number">3.18542421e-02</span>,</span><br><span class="line">          <span class="number">-1.29441142e-01</span>, <span class="number">-4.51699793e-02</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">1.38353303e-01</span>, <span class="number">-1.54019877e-01</span>,  <span class="number">3.47476661e-01</span>,</span><br><span class="line">           <span class="number">3.10183734e-01</span>,  <span class="number">9.80487838e-02</span>, <span class="number">-8.70270059e-02</span>,</span><br><span class="line">          <span class="number">-3.14944327e-01</span>,  <span class="number">3.03581864e-01</span>,  <span class="number">1.18549809e-01</span>,</span><br><span class="line">           <span class="number">4.82388325e-02</span>, <span class="number">-4.62612629e-01</span>,  <span class="number">1.56953588e-01</span>,</span><br><span class="line">           <span class="number">1.20078214e-01</span>,  <span class="number">2.14997604e-01</span>,  <span class="number">2.20677666e-02</span>,</span><br><span class="line">          <span class="number">-1.07790619e-01</span>, <span class="number">-4.78487350e-02</span>, <span class="number">-1.66399360e-01</span>,</span><br><span class="line">          <span class="number">-3.04439962e-01</span>, <span class="number">-1.03561424e-01</span>,  <span class="number">1.27254218e-01</span>,</span><br><span class="line">           <span class="number">2.68438198e-02</span>, <span class="number">-3.16898555e-01</span>, <span class="number">-1.08468547e-01</span>,</span><br><span class="line">           <span class="number">4.99541610e-02</span>, <span class="number">-3.13619196e-01</span>, <span class="number">-3.84023368e-01</span>,</span><br><span class="line">           <span class="number">1.85616702e-01</span>, <span class="number">-1.11300424e-01</span>, <span class="number">-2.30957553e-01</span>,</span><br><span class="line">          <span class="number">-8.71629417e-02</span>, <span class="number">-2.18425304e-01</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">5.16243160e-01</span>, <span class="number">-1.59118176e-01</span>,  <span class="number">2.56632596e-01</span>,</span><br><span class="line">           <span class="number">2.31499642e-01</span>,  <span class="number">3.12861830e-01</span>, <span class="number">-1.93787217e-01</span>,</span><br><span class="line">          <span class="number">-3.51059884e-02</span>, <span class="number">-1.76955208e-01</span>, <span class="number">-1.34864435e-01</span>,</span><br><span class="line">           <span class="number">1.62184462e-01</span>,  <span class="number">1.46136060e-01</span>,  <span class="number">2.63225555e-01</span>,</span><br><span class="line">           <span class="number">1.66042566e-01</span>,  <span class="number">1.86691016e-01</span>, <span class="number">-2.67370224e-01</span>,</span><br><span class="line">          <span class="number">-5.90280676e-03</span>,  <span class="number">2.70183504e-01</span>,  <span class="number">5.89993186e-02</span>,</span><br><span class="line">          <span class="number">-2.57559538e-01</span>, <span class="number">-4.12272394e-01</span>, <span class="number">-8.70700553e-02</span>,</span><br><span class="line">           <span class="number">1.34470537e-01</span>, <span class="number">-1.67689323e-01</span>, <span class="number">-3.41424406e-01</span>,</span><br><span class="line">           <span class="number">2.84074187e-01</span>, <span class="number">-1.07147172e-01</span>,  <span class="number">1.03664622e-01</span>,</span><br><span class="line">          <span class="number">-2.63156950e-01</span>, <span class="number">-3.71470511e-01</span>, <span class="number">-2.90910363e-01</span>,</span><br><span class="line">           <span class="number">1.30605936e-01</span>, <span class="number">-4.24639434e-01</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">4.88423556e-02</span>,  <span class="number">3.35189223e-01</span>,  <span class="number">2.53643900e-01</span>,</span><br><span class="line">          <span class="number">-2.06524417e-01</span>, <span class="number">-3.78751934e-01</span>,  <span class="number">1.34045901e-02</span>,</span><br><span class="line">           <span class="number">1.64140776e-01</span>, <span class="number">-1.75927207e-01</span>, <span class="number">-1.66527703e-01</span>,</span><br><span class="line">           <span class="number">3.91573310e-01</span>,  <span class="number">2.06493884e-01</span>,  <span class="number">2.74771124e-01</span>,</span><br><span class="line">           <span class="number">1.69764921e-01</span>, <span class="number">-4.06905115e-01</span>, <span class="number">-3.35692346e-01</span>,</span><br><span class="line">           <span class="number">8.99787545e-02</span>, <span class="number">-5.60279638e-02</span>,  <span class="number">3.24890286e-01</span>,</span><br><span class="line">          <span class="number">-6.52969703e-02</span>, <span class="number">-3.02911997e-01</span>, <span class="number">-1.91059798e-01</span>,</span><br><span class="line">           <span class="number">2.11907119e-01</span>,  <span class="number">6.60512149e-02</span>, <span class="number">-6.08844198e-02</span>,</span><br><span class="line">           <span class="number">1.97099358e-01</span>, <span class="number">-4.50475607e-03</span>,  <span class="number">3.00196797e-01</span>,</span><br><span class="line">          <span class="number">-1.27629504e-01</span>, <span class="number">-2.09806144e-01</span>, <span class="number">-1.09069586e-01</span>,</span><br><span class="line">           <span class="number">1.95026800e-01</span>, <span class="number">-2.41192505e-01</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">-4.21599858e-02</span>, <span class="number">-1.33833423e-01</span>,  <span class="number">1.73788846e-01</span>,</span><br><span class="line">          <span class="number">-8.74219313e-02</span>,  <span class="number">1.79445803e-01</span>,  <span class="number">2.18065772e-02</span>,</span><br><span class="line">           <span class="number">1.11098528e-01</span>,  <span class="number">2.27415338e-01</span>,  <span class="number">1.67024761e-01</span>,</span><br><span class="line">           <span class="number">1.86085999e-01</span>,  <span class="number">6.45482726e-03</span>,  <span class="number">1.15672685e-01</span>,</span><br><span class="line">           <span class="number">9.32103917e-02</span>,  <span class="number">4.86121178e-02</span>, <span class="number">-1.34105667e-01</span>,</span><br><span class="line">           <span class="number">2.61788070e-01</span>, <span class="number">-1.33057624e-01</span>,  <span class="number">2.08185226e-01</span>,</span><br><span class="line">           <span class="number">1.11163380e-02</span>, <span class="number">-1.39638558e-01</span>,  <span class="number">3.82541418e-02</span>,</span><br><span class="line">           <span class="number">2.05305278e-01</span>,  <span class="number">2.74346590e-01</span>,  <span class="number">1.07309587e-01</span>,</span><br><span class="line">           <span class="number">1.53153718e-01</span>, <span class="number">-7.80853480e-02</span>, <span class="number">-1.45936459e-01</span>,</span><br><span class="line">           <span class="number">1.15661368e-01</span>,  <span class="number">2.84138117e-02</span>,  <span class="number">2.60127039e-04</span>,</span><br><span class="line">           <span class="number">2.58475721e-01</span>,  <span class="number">2.48304214e-02</span>]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">       [[[<span class="number">-9.96622518e-02</span>,  <span class="number">1.37207210e-01</span>,  <span class="number">6.49073049e-02</span>,</span><br><span class="line">          <span class="number">-2.56701887e-01</span>, <span class="number">-1.92326424e-03</span>, <span class="number">-1.43860981e-01</span>,</span><br><span class="line">           <span class="number">1.09956741e-01</span>,  <span class="number">1.07560353e-02</span>,  <span class="number">3.54471877e-02</span>,</span><br><span class="line">          <span class="number">-7.16849370e-03</span>,  <span class="number">2.60522217e-01</span>, <span class="number">-4.55740020e-02</span>,</span><br><span class="line">           <span class="number">6.92887977e-02</span>, <span class="number">-2.51750741e-02</span>,  <span class="number">1.67305902e-01</span>,</span><br><span class="line">           <span class="number">3.12767439e-02</span>, <span class="number">-1.02101907e-01</span>, <span class="number">-1.38990544e-02</span>,</span><br><span class="line">          <span class="number">-5.27952351e-02</span>, <span class="number">-5.41742854e-02</span>,  <span class="number">9.97792631e-02</span>,</span><br><span class="line">           <span class="number">1.18075432e-02</span>, <span class="number">-9.80602354e-02</span>, <span class="number">-7.14249611e-02</span>,</span><br><span class="line">          <span class="number">-2.99854353e-02</span>, <span class="number">-3.63657847e-02</span>,  <span class="number">2.59663343e-01</span>,</span><br><span class="line">           <span class="number">1.45671824e-02</span>,  <span class="number">2.26698264e-01</span>, <span class="number">-4.12496440e-02</span>,</span><br><span class="line">          <span class="number">-1.07028335e-01</span>,  <span class="number">1.12535998e-01</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">-3.71173561e-01</span>, <span class="number">-3.47134918e-01</span>, <span class="number">-1.25734866e-01</span>,</span><br><span class="line">          <span class="number">-1.04274608e-01</span>, <span class="number">-1.17725596e-01</span>, <span class="number">-4.32399511e-02</span>,</span><br><span class="line">          <span class="number">-1.25010118e-01</span>,  <span class="number">1.28871411e-01</span>, <span class="number">-1.16636446e-02</span>,</span><br><span class="line">           <span class="number">3.53644863e-02</span>, <span class="number">-3.43158126e-01</span>, <span class="number">-6.99633360e-02</span>,</span><br><span class="line">          <span class="number">-4.92055565e-02</span>, <span class="number">-2.04492901e-02</span>,  <span class="number">2.45652124e-01</span>,</span><br><span class="line">          <span class="number">-2.31047466e-01</span>, <span class="number">-2.06009001e-01</span>,  <span class="number">3.27508003e-02</span>,</span><br><span class="line">          <span class="number">-1.18006527e-01</span>,  <span class="number">2.33523156e-02</span>,  <span class="number">4.04132828e-02</span>,</span><br><span class="line">          <span class="number">-1.25319809e-01</span>, <span class="number">-2.09883824e-01</span>, <span class="number">-1.37493312e-01</span>,</span><br><span class="line">           <span class="number">2.33293012e-01</span>, <span class="number">-1.44781396e-02</span>, <span class="number">-2.18876049e-01</span>,</span><br><span class="line">           <span class="number">1.32711887e-01</span>,  <span class="number">3.90832350e-02</span>, <span class="number">-2.01453134e-01</span>,</span><br><span class="line">          <span class="number">-4.23350960e-01</span>,  <span class="number">1.45124316e-01</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">-5.36839813e-02</span>,  <span class="number">1.58995256e-01</span>, <span class="number">-1.83390737e-01</span>,</span><br><span class="line">           <span class="number">3.47549677e-01</span>,  <span class="number">1.38828710e-01</span>,  <span class="number">4.24572304e-02</span>,</span><br><span class="line">          <span class="number">-1.27515435e-01</span>, <span class="number">-9.66101289e-02</span>, <span class="number">-1.87338620e-01</span>,</span><br><span class="line">          <span class="number">-1.97422896e-02</span>, <span class="number">-1.08366840e-01</span>, <span class="number">-1.55322403e-01</span>,</span><br><span class="line">          <span class="number">-1.61157817e-01</span>,  <span class="number">2.01927379e-01</span>,  <span class="number">2.53322780e-01</span>,</span><br><span class="line">          <span class="number">-5.62116742e-01</span>, <span class="number">-5.91066889e-02</span>,  <span class="number">5.40728793e-02</span>,</span><br><span class="line">          <span class="number">-7.92302191e-02</span>,  <span class="number">1.88086838e-01</span>,  <span class="number">6.59958273e-03</span>,</span><br><span class="line">          <span class="number">-2.42399514e-01</span>,  <span class="number">1.69646740e-01</span>, <span class="number">-9.47589334e-03</span>,</span><br><span class="line">           <span class="number">1.11526139e-01</span>,  <span class="number">6.88120276e-02</span>, <span class="number">-9.78308246e-02</span>,</span><br><span class="line">          <span class="number">-1.60721883e-01</span>,  <span class="number">6.95262775e-02</span>, <span class="number">-6.26887083e-02</span>,</span><br><span class="line">          <span class="number">-2.41015360e-01</span>, <span class="number">-2.02056006e-01</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">1.95601895e-01</span>,  <span class="number">1.46274611e-01</span>, <span class="number">-2.14165911e-01</span>,</span><br><span class="line">           <span class="number">1.88677847e-01</span>, <span class="number">-6.13280199e-03</span>,  <span class="number">3.82143594e-02</span>,</span><br><span class="line">          <span class="number">-2.77706925e-02</span>, <span class="number">-2.12541133e-01</span>,  <span class="number">3.08963843e-02</span>,</span><br><span class="line">          <span class="number">-2.32768711e-02</span>,  <span class="number">4.44432311e-02</span>, <span class="number">-7.64122903e-02</span>,</span><br><span class="line">          <span class="number">-2.42977053e-01</span>,  <span class="number">2.11474299e-02</span>,  <span class="number">3.08647826e-02</span>,</span><br><span class="line">          <span class="number">-2.96457887e-01</span>,  <span class="number">2.82384634e-01</span>, <span class="number">-9.74976271e-02</span>,</span><br><span class="line">          <span class="number">-1.13737859e-01</span>,  <span class="number">3.41809720e-01</span>, <span class="number">-2.39258468e-01</span>,</span><br><span class="line">          <span class="number">-3.60288233e-01</span>,  <span class="number">1.55391827e-01</span>,  <span class="number">1.84775457e-01</span>,</span><br><span class="line">           <span class="number">7.04837516e-02</span>,  <span class="number">1.14113115e-01</span>,  <span class="number">1.53539777e-01</span>,</span><br><span class="line">          <span class="number">-1.83223680e-01</span>,  <span class="number">8.52515474e-02</span>,  <span class="number">4.64094169e-02</span>,</span><br><span class="line">          <span class="number">-2.26785466e-02</span>, <span class="number">-3.02122593e-01</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">4.71174717e-02</span>, <span class="number">-1.33249089e-01</span>, <span class="number">-7.07437098e-02</span>,</span><br><span class="line">          <span class="number">-1.64638892e-01</span>,  <span class="number">5.91888167e-02</span>,  <span class="number">6.15437292e-02</span>,</span><br><span class="line">           <span class="number">2.02578202e-01</span>,  <span class="number">3.63162197e-02</span>,  <span class="number">4.95729856e-02</span>,</span><br><span class="line">          <span class="number">-1.76949397e-01</span>,  <span class="number">8.75761509e-02</span>, <span class="number">-2.57797502e-02</span>,</span><br><span class="line">          <span class="number">-2.62792528e-01</span>, <span class="number">-1.61450431e-01</span>, <span class="number">-1.10014781e-01</span>,</span><br><span class="line">          <span class="number">-3.13372947e-02</span>,  <span class="number">9.80259553e-02</span>, <span class="number">-8.82068127e-02</span>,</span><br><span class="line">           <span class="number">4.45931666e-02</span>,  <span class="number">1.94095060e-01</span>, <span class="number">-1.57402858e-01</span>,</span><br><span class="line">          <span class="number">-1.39938518e-01</span>,  <span class="number">2.33131334e-01</span>,  <span class="number">3.16902958e-02</span>,</span><br><span class="line">           <span class="number">2.28416715e-02</span>,  <span class="number">6.12144656e-02</span>, <span class="number">-9.37361270e-03</span>,</span><br><span class="line">           <span class="number">1.28603980e-01</span>, <span class="number">-7.55553991e-02</span>,  <span class="number">1.83364391e-01</span>,</span><br><span class="line">           <span class="number">1.22262053e-01</span>, <span class="number">-4.33576889e-02</span>]]]], dtype=float32), array([ <span class="number">0.06133625</span>, <span class="number">-0.06743222</span>,  <span class="number">0.12276675</span>,  <span class="number">0.25595677</span>,  <span class="number">0.01088508</span>,</span><br><span class="line">        <span class="number">0.16797405</span>,  <span class="number">0.12758622</span>, <span class="number">-0.3513619</span> , <span class="number">-0.08484235</span>, <span class="number">-0.0949592</span> ,</span><br><span class="line">        <span class="number">0.04725884</span>, <span class="number">-0.17716943</span>, <span class="number">-0.05790677</span>,  <span class="number">0.04177701</span>, <span class="number">-0.12494741</span>,</span><br><span class="line">        <span class="number">0.39589486</span>, <span class="number">-0.0406141</span> , <span class="number">-0.11065134</span>, <span class="number">-0.04198102</span>, <span class="number">-0.10928375</span>,</span><br><span class="line">        <span class="number">0.06801659</span>,  <span class="number">0.09649571</span>,  <span class="number">0.34070975</span>, <span class="number">-0.10038836</span>, <span class="number">-0.04970933</span>,</span><br><span class="line">        <span class="number">0.01176555</span>, <span class="number">-0.18670177</span>, <span class="number">-0.0030286</span> ,  <span class="number">0.05685186</span>, <span class="number">-0.18897383</span>,</span><br><span class="line">        <span class="number">0.16669714</span>,  <span class="number">0.1053075</span> ], dtype=float32)]</span><br></pre></td></tr></table></figure>
<p>每一块是32个，一共26块，前面是weight，后面是bias吧…</p>
<p>将training data第一张图丢进第一个卷积层中得到的结果</p>
<div class="img-wrap">
<img src="/img/ml-practice/hw3/showpic0.jpg">
<img src="/img/ml-practice/hw3/showpic1.jpg">
<img src="/img/ml-practice/hw3/showpic2.jpg">
<img src="/img/ml-practice/hw3/showpic3.jpg">
<img src="/img/ml-practice/hw3/showpic4.jpg">
</div>

<div class="img-wrap">
<img src="/img/ml-practice/hw3/showpic5.jpg">
<img src="/img/ml-practice/hw3/showpic6.jpg">
<img src="/img/ml-practice/hw3/showpic7.jpg">
<img src="/img/ml-practice/hw3/showpic8.jpg">
<img src="/img/ml-practice/hw3/showpic9.jpg">
</div>

<div class="img-wrap">
<img src="/img/ml-practice/hw3/showpic10.jpg">
<img src="/img/ml-practice/hw3/showpic11.jpg">
<img src="/img/ml-practice/hw3/showpic12.jpg">
<img src="/img/ml-practice/hw3/showpic13.jpg">
<img src="/img/ml-practice/hw3/showpic14.jpg">
</div>

<div class="img-wrap">
<img src="/img/ml-practice/hw3/showpic15.jpg">
<img src="/img/ml-practice/hw3/showpic16.jpg">
<img src="/img/ml-practice/hw3/showpic17.jpg">
<img src="/img/ml-practice/hw3/showpic18.jpg">
<img src="/img/ml-practice/hw3/showpic19.jpg">
</div>

<div class="img-wrap">
<img src="/img/ml-practice/hw3/showpic20.jpg">
<img src="/img/ml-practice/hw3/showpic21.jpg">
<img src="/img/ml-practice/hw3/showpic22.jpg">
<img src="/img/ml-practice/hw3/showpic23.jpg">
<img src="/img/ml-practice/hw3/showpic24.jpg">
</div>

<div class="img-wrap">
<img src="/img/ml-practice/hw3/showpic25.jpg">
<img src="/img/ml-practice/hw3/showpic26.jpg">
<img src="/img/ml-practice/hw3/showpic27.jpg">
<img src="/img/ml-practice/hw3/showpic28.jpg">
<img src="/img/ml-practice/hw3/showpic29.jpg">
</div>

<p><div class="img-wrap">
<img src="/img/ml-practice/hw3/showpic30.jpg">
<img src="/img/ml-practice/hw3/showpic31.jpg">
<img src="/img/ml-practice/hw3/pic0.jpg">
<img src="/img/ml-practice/hw3/showpic32.jpg">
<img src="/img/ml-practice/hw3/showpic32.jpg">
</div><br><br></p>
<p><del>第32张为什么加载不出来？？？？？</del><br>最后一张是原图，可以看出来有些感觉就是变成惊讶或者悲伤的样子的，有些特别处理了眉毛，有些特别处理了眼睛的样子吧..<br>(纯属口胡，我觉得我该好好看看相关论文了</p>
<p>相关代码<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show</span><span class="params">(model, x_train, y_train)</span>:</span></span><br><span class="line">    <span class="comment">#print(model.get_layer('conv2d_1').get_weights())</span></span><br><span class="line">    layer_model=Model(inputs=model.input,outputs=model.layers[<span class="number">0</span>].output)</span><br><span class="line">    t = layer_model.predict(x_train)</span><br><span class="line">    print(t, t.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">32</span>):</span><br><span class="line">        plt.imshow(t[<span class="number">0</span>, :, :, i])</span><br><span class="line">        plt.axis(<span class="string">'off'</span>)</span><br><span class="line">        plt.savefig(<span class="string">'showpic'</span>+str(i)+<span class="string">'.jpg'</span>)</span><br><span class="line">        plt.show()</span><br></pre></td></tr></table></figure></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/python/" rel="tag"><i class="fa fa-tag"></i> python</a>
          
            <a href="/tags/problem/" rel="tag"><i class="fa fa-tag"></i> problem</a>
          
            <a href="/tags/machinelearning/" rel="tag"><i class="fa fa-tag"></i> machinelearning</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/07/13/note-pwn/" rel="next" title="pwn学习笔记">
                <i class="fa fa-chevron-left"></i> pwn学习笔记
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/07/22/20190722/" rel="prev" title="20190722">
                20190722 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div class="comments" id="comments">
        <div id="gitalk-container"></div>
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="北屿">
          <p class="site-author-name" itemprop="name">北屿</p>
           
              <p class="site-description motion-element" itemprop="description">Sometimes it's the very people who no one imagines angthing of who do the things that no one can imagine.</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">63</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">11</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">58</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/beiyuouo" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                    
                      GitHub
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://weibo.com/u/5687969852" target="_blank" title="Weibo">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                    
                      Weibo
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://wpa.qq.com/msgrd?v=3&uin=729320011&site=qq&menu=yes" target="_blank" title="QQ">
                  
                    <i class="fa fa-fw fa-qq"></i>
                  
                    
                      QQ
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.zhihu.com/people/bei-yu-84-25/activities" target="_blank" title="ZhiHu">
                  
                    <i class="fa fa-fw fa-unlink"></i>
                  
                    
                      ZhiHu
                    
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-block">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              Links
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://baidu.com/" title="欢迎友链qwq" target="_blank">欢迎友链qwq</a>
                </li>
              
            </ul>
          </div>
        

        

      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#hw1"><span class="nav-number">1.</span> <span class="nav-text">hw1</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#hw2"><span class="nav-number">2.</span> <span class="nav-text">hw2</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Probabilistic-Generateive-Model"><span class="nav-number">2.1.</span> <span class="nav-text">Probabilistic Generateive Model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Logistic-Regression-Model"><span class="nav-number">2.2.</span> <span class="nav-text">Logistic Regression Model</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#hw3"><span class="nav-number">3.</span> <span class="nav-text">hw3</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Model"><span class="nav-number">3.1.</span> <span class="nav-text">Model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Data"><span class="nav-number">3.2.</span> <span class="nav-text">Data</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">
  
  &copy;  2018 - 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">北屿</span>
</div>


<div class="powered-by">
  Powered by <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>


        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>


        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>

  
  <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.2"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  




	





  





  







  <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
  <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
  <script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.js"></script>
   <script type="text/javascript">
        var gitalk = new Gitalk({
          clientID: '3bd523d04a7fcd807bd1',
          clientSecret: '3c4d80f8fd534840b0571ebf194825129668c1fe',
          repo: 'gitalk',
          owner: 'beiyuouo',
          admin: ['beiyuouo'],
          id: md5(location.pathname),
          abels: 'gitalk'.split(',').filter(l => l),
          perPage: 10,
          pagerDirection: 'last',
          createIssueManually: true,
          distractionFreeMode: false
        })
        gitalk.render('gitalk-container')           
       </script>




  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

#
#<script type="text/javascript" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"></script>
#<!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>
